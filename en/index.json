[
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/rw_data_guide/mql_vs_sql/",
	"title": "&#39;MQL&#39; vs. SQL",
	"tags": [],
	"description": "&#39;MQL&#39; vs. SQL",
	"content": " Call a function; don't write a statement 1. There is no MongoDB query language The \u0026quot;L\u0026quot; in SQL is \u0026quot;Language\u0026quot;. A syntax that defines how you can write a sentence such as \u0026quot;SELECT usr_id, COUNT(*) AS count FROM TableA GROUP BY usr_id\u0026quot; that can be parsed to become query or update plans.\nMongoDB driver API's don't require that. They don't require you to 'program' a piece of text for the remote database server to parse (and potentially throw a syntax error on).\n2. Call a function and give it arguments. Depending on which language you are programming your client application in the API will be different, but the common thing is that you make all database requests by calling separate functions for the matching database commands.\nHere are some examples of doing the insert command, covering one OOP and one imperative style language:\n//Python my_collection.insert_one(doc) //C mongoc_collection_insert_one(my_collection_pointer, doc_pointer, NULL, NULL, \u0026amp;err);  The above use the API provided by PyMongo and mongoc MongoDB drivers. If you use Python or C you can see the syntax is very normal for them. Likewise the Java driver is idiomatic for Java, the NodeJS driver is idiomatic for NodeJS, the C# driver is idiomatic for C#, etc.\nA database-side command of xXxx will be called by executing the API function probably named xXxx(..) or Xxxx(..) or mongo_xxxx(..), etc. It might be a synonym too, e.g. \u0026quot;find\u0026quot; \u0026lt;--\u0026gt; \u0026quot;query\u0026quot; or \u0026quot;delete\u0026quot; \u0026lt;--\u0026gt; \u0026quot;remove\u0026quot;.\nSome of the functions in the driver API are extra wrappers that provide some kind of syntactic convenience. The insert_one(..) function and its partner insert_many([..]) are an example. Underneath both is just one command, insert, which requires that the document(s) being inserted be passed in an array. insert_one saves you the tiny bit of boilerplate work of instantiating an array to put your one document inside.\nIn one sense db and collection names are arguments too In the example above there are two arguments, although the OOP example especially makes it easy to miss. There is the collection that will be inserted to, plus the doc that will be inserted.\nA database namespace will always be in scope too. Database name is typically set when you created the db connection so it wont be in the function-calling code line, but the driver will be packing it in every request for you behind the scenes.\nAs a matter of semantics you might call both the database and collection scope or you might call them arguments.\nI would say: In the wire protocol packed format they are just string names (argument), but in the code they are scope.\nOn the driver side the scope is kept mostly for tracking which db name and collection to pack in requests. On the server-side the db or collection name strings received from a wire protocol request are immediately used to reopen or instantiate memory structures with pointers to existing collection names for a given db, pointers to existing indexes for a collection, etc.\n3. Receive the result Nothing new here for any who has programmed a database-using application before. When you call the function there will be a call over the network to the MongoDB server and for any given function in the API there will be one type of result.\nTo choose the simplest example this is the response from some deletes executed using PyMongo. The response confirms they ran OK; in the case of the second it shows that 5 documents matched the filter clause and were deleted.\n... \u0026gt;\u0026gt;\u0026gt; del_result = posts.delete_one({}) \u0026gt;\u0026gt;\u0026gt; del_result \u0026lt;pymongo.results.DeleteResult object at 0x7f7580fa9ec8\u0026gt; \u0026gt;\u0026gt;\u0026gt; del_result.raw_result {'n': 1, 'ok': 1.0} \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; del_result = posts.delete_many({\u0026quot;z\u0026quot;: \u0026quot;test string\u0026quot;}) \u0026gt;\u0026gt;\u0026gt; del_result.raw_result {'n': 5, 'ok': 1.0}  Next example: The find (or however it is spelled) function returns a cursor object which can be used to iterate the 0 or more BSON documents the server matched in the query. A find_one (a.k.a. findOne) doesn't oblige to you to first take the cursor reference, then enter a loop to get the BSON results. Instead the result is a single BSON document (or null), once again saving you a little boilerplate code for the common case when you know you should only have one result (or don't care about supplementaries).\nThe various result types that exist are an interesting topic ... no, they're not. I lied. Whichever driver API you're using it is intuitive, but unchallenging and uninteresting as a result. Cursors iterate documents, writes return write results, the 'list database / collection / index names' functions return a cursor of those names, count returns a integer, etc., etc.\nRPC-ish More like an RPC-using lib than a language People who call it \u0026quot;MQL\u0026quot; reminds us of the 19th century people who used the term \u0026quot;horseless carriage\u0026quot; for the newly-invented automobile. Terms from incompatible old concepts were recycled to describe the new ones.\nThe automobile was truly something new. But a MongoDB driver is not - it packs requests as command objects (your client program language's data -\u0026gt; BSON command object) and an 'X' command is executed through the matching function just for 'X' in the mongod server code. It's not generic enough to be an RPC, but it's something like that.\nThe 'language' is knowing the function reference You can't just put anything in the X(...) function of course. It expects certain input, some required and some optional, and without the required arguments the driver will reject it even before sending it. At compile time, if it's a compiled language.\nFor example an update command in it's canonical, MongoDB Wire Protocol and server-side format has the following fields:\n db + collection namespace (set via the scope of a collection object; a collection object pointer; etc.) An array of one or more composite update objects with these fields:  A filter object to find the document(s) to update The update modifications Optional: upsert true/false Optional: \u0026quot;multi\u0026quot; true/false Optional: collation Optional: filters controlling which nested array items can be affected  Optional: ordered processing only true/false Optional: writeConcern Optional: bypassDocumentValidation true/false  Whichever programming language you are using the MongoDB driver API for it will, in it's update function (or update_one and update_many functions), be setting these same required fields and optionally setting the optional fields.\nYour job as the MongoDB-using programmer 1. Memorize the basic, required arguments. E.g. that an update needs at least two arguments on top of it's collection namespace scope - one \u0026quot;filter\u0026quot; argument that finds the doc(s) to update; another to set the new value(s) in it.\n2. Get familiar with the object-packing format of arguments When an argument is a scalar value, whether a universal programming datatype such as a string, number, boolean, or MongoDB extended type such as Datetime, ObjectId, etc., its obvious how to pass that.\nBut when an argument is an object (such as query filter, update modification rule, shard zone tag range) that's not obvious because those design choices are arbitrary. (This applies to any language/system; this is not a MongoDB-specific issue).\n//mongo shell javascript example db.my_collection.find({\u0026quot;x\u0026quot;: {\u0026quot;$lt\u0026quot;: 100}}, {\u0026quot;_id\u0026quot;: true, \u0026quot;dept\u0026quot;: true})  The line above is equivalent to \u0026quot;SELECT _id, dept FROM my_collection WHERE x \u0026lt; 100\u0026quot;.\nThe command-delimted field name list \u0026quot;_id, dept\u0026quot; directly after the keyword \u0026quot;SELECT\u0026quot; in the SQL query becomes {\u0026quot;_id\u0026quot;: true, \u0026quot;dept\u0026quot;: true}. What was \u0026quot;x \u0026lt; 100\u0026quot; put after the \u0026quot;WHERE\u0026quot; keyword in the SQL becomes {\u0026quot;x\u0026quot;: {\u0026quot;$lt\u0026quot;: 100}}.\nLearning the right order of the arguments is trivial, but the choices made on how to represent filter clauses, field lists, update operations are not necessarily the first one you might guess.\nSo learn the query filter, field projection, update specification and index specification documents so you have fluent recall of them. The aggregation pipeline stages and operators are also deserve studying until you can at least remember $match, $project and $group fluently too.\nThe query filter, update specification and aggregation pipeline stages have more-commonly used operators (e.g. $in, $sum, $set) and less-commonly used ones (too many to list). For the less common operatos and other object types (e.g. zoning shard tag ranges) just look them up on the rare occasions you use them.\n3. Keep the optional stuff in your fuzzy memory On your first read do make an point of skimming all the options to find things that are novel or otherwise surprising. Leave those neat tricks (or gotchas) in your subconcious so it can guide you later.\nAdvanced study Notice when client-side is presenting a pretty picture differing from the server command reality Most of the client-side functions match up to a single server command with arguments that are exactly the same. rs.status() -\u0026gt; replSetGetStatus, db.createUser(...) -\u0026gt; createUser, etc. Sometimes it a different command name and\nBut in other cases, particularly for the most-used commands, there are differences in what you see with your programming language's MongoDB driver API vs. the db server-side command spec.\nE.g. 1. db.setLogLevel(1, \u0026quot;network\u0026quot;) -\u0026gt; { setParameter: 1, logComponentVerbosity: {\u0026quot;network\u0026quot;: 1} }\nE.g. 2. You might seen in tutorials that the insert, update and delete methods in your API's examples were being fed single documents. In truth their matching server commands (insert, update and delete) take arrays of the new insert / update specification / delete filter docs.\nYour (recent version) MongoDB driver has insert-one or insert-many, update-one or update-many, and delete-one or delete-many functions though.\nIf you're aware that these are really calling the same server-side command it'll prevent you from various worries you might have. For example don't be concerned that calling insert_many([new_doc]) with a single item in the array is going to have worse performance. It's exactly the same thing as insert_one(new_doc) to the server side.\n"
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/connecting_guide/human_ix/conn_string_uri/",
	"title": "Connection string URI",
	"tags": [],
	"description": "MongoDB connection URI syntax",
	"content": " Ladies and gentlemen, I present to you (drum roll) the standard MongoDB connection string URI:\nmongodb://[user[:password]@]host[:port][,host[:port]]*[/[database_name][?[conn_option[=value]][,conn_option[=value]]*]]\nAt this point I expect all readers fall into one of following three groups:\n 'Yuck. Punctuation vomit or what?' 'Ah, a URI like those ones for ODBC / Postgresql / MS SQL server / MySQL etc. ...' 'You've neglected the DNS/SRV seedlist format, moron'  If you are in group #1 I'm sorry but I can't 'sexy it up' in any way, no matter how hard I try. At it's simplest the URI you type is short and easy, but as you add authentication credentials and options it can't help becoming more and more verbose.\nIf you are in group #2 one special difference to note is that multiple host+port tuples can be accepted, instead of just one. More on this in the Replicaset host-list syntax section below.\nIf you are in group #3, cool, you're done, you don't need this page and you can move onto the next. (And I'll say I'm envious that you've been given enough privileges to add and modify SRV records on your DNS servers.)\nNon-URI formats You may have used (or will see on other sites) ways to connect without using the URI format. E.g. with the shell mongo --host myhost.my.domain:27018, or a code sample something like var conn = new MongoClient(\u0026quot;myhost.my.domain\u0026quot;, 27018)'.\nThese are just for legacy compatibility and/or to give some abbreviation. In reality those arguments will be immediately reformatted into a new 'mongodb://...' string and that is what the driver code will use.\nAs well as being deprecated the non-URI formats differ in syntax from one language driver to another. Let's just stop thinking about them a.s.a.p.\nExamples Basic / minimal To make a new connection a MongoDB client needs at least the two things any TCP connection requires - a hostname (or it's IP address) and a port.\nmongodb://myhost.my.domain:27017/\nWhat if host or port are wrong or the MongoDB server can't be reached because of a network problem? The TCP connection will never be established and the error message will be something along those lines. E.g. 'socket exception', and not 'MongoDB server failure'.\nAdding access credentials If the DB requires users to authenticate with username and password then add those too, delimited by \u0026quot;:\u0026quot; and suffixed with \u0026quot;@\u0026quot;.\nmongodb://akira:secret@myhost.my.domain:27017/\nIf you have tricky punctuation characters in your password that would wreck the URI parsing (i.e. \u0026quot;/\u0026quot;, \u0026quot;:\u0026quot;, \u0026quot;@\u0026quot;, or \u0026quot;%\u0026quot;) encode those with percent encoding. E.g. \u0026quot;EatMyH@t\u0026quot; -\u0026gt; \u0026quot;EatMyH%40t\u0026quot;.\n By default/convention the user authentication credentials are saved in the \u0026quot;admin\u0026quot; db on the server. This is the assumed default for mongodb connection URIs too, so you can leave it absent (as above) most of the time.\nBut if the \u0026quot;akira\u0026quot; user authentication credentials had been created in in the \u0026quot;orderhist\u0026quot; user databases then that db name is needed as shown below. The first format sets the starting db namespace (that commands such as find, db stats, etc. will act in) as \u0026quot;orderhist\u0026quot; and the auth source db is assumed to be the same. The second format allows for the starting db namespace to be something else.\nmongodb://akira:secret@myhost.my.domain:27017/orderhist\nmongodb://akira:secret@myhost.my.domain:27017/[some_other_db]?authSource=orderhist\nI do not recommended created user auth outside the \u0026quot;admin\u0026quot; db \u0026ndash; the above is just for reference in case you are accessing a non-conventional MongoDB cluster or replica set.\n What if the user credentials are rejected (e.g. unknown username or wrong password)? The TCP socket connection will be established for a moment. Over the TCP connection the username and it's hashed password will be sent. If they fail the server will send the failure reply ('user unauthorized', etc.) in a MongoDB Wire protocol OP_REPLY (or rereply OP_MSG?), then close the socket immediately.\nQ. \u0026quot;What if the MongoDB server requires user authentication but the client fails to give username and password?\u0026quot;\nUnintuitively the TCP connection will be established and stay open! It will remain open to allow the client to send db user credentials. Any command other than authenticate or the ones drivers need for basic state detection (isMaster, hostInfo, etc.) will be rejected with an authorization error.\nConnection options For convenience the full syntax again:\nmongodb://[user[:password]@]host[:port][,host[:port]]*[/[database_name][?[conn_option[=value]][,conn_option[=value]]*]]\nAfter the \u0026quot;/\u0026quot; that follows host[:port] all the values are optional. The first is the user_auth_db_name (see above), then whether that db name is present or not put \u0026quot;?\u0026quot; before any other parameters. Delimit with an \u0026quot;\u0026amp;\u0026quot;, like in a HTTP URI.\nExample If the DB was configured to accept connections that use SSL network encryption then from the client side we add the \u0026quot;ssl=true\u0026quot; to instruct the driver to do that. If we want the driver to make a pool of at least 50 database connections that different threads in the application can share, then we could add \u0026quot;minPoolSize=50\u0026quot;.\nmongodb://akira:secret@myhost.my.domain:27017/?ssl=true\u0026amp;minPoolSize=50\nStaring in v4.2 there is general renaming of \u0026quot;SSL\u0026quot; as \u0026quot;TLS\u0026quot; in mongod/mongos server node and mongo shell options, and the MongoDB documentation in general (link). It seems the connection string URI options are going to remain ssl* though.\n Most common options ... as I recall seeing / expect should be used.\n replicaSet. Invalid if connecting to a sharded cluster. But otherwise use this ensure you establish a replicaset connection that will failover in the event of a primary switch, rather than just having a standalone connection. authSource readPreference N.b. I'd recommend not using this; i.e. always use the default, which is doing reads only from primaries. But I know many users do secondary reads (I hypothesize a habit learned from completely different, non-db systems that have load balancers) and this is the option for setting that. w (write concern level) connectTimeoutMS retryWrites If there is a primary switch this will automatically retry once (but only once) any writes to the old primary that errored because it crashed / was stepped down in the middle of the first attempt.  The (one) connection parameter that can't go in the URI Using the \u0026quot;ssl=true\u0026quot; option in my example above requires me to point out that, unfortunately, one of the SSL options (but so far only this one) need to be passed to the client connection function outside of URI. SSL connections usually require a CA cert file and/or a client certificate file to be used, and you can't put a local filepath into an URI. (Well maybe you can, but this isn't standardizedyet.)\nA C++ driver example of adding an SSL client PEM file is shown below. The point is simply that the minimal case of needing just one line with the URI connection in it is no longer possible; we have to prepare and add an extra connection option alongside it when the client connection object is constructed.\nmongocxx::options::ssl ssl_opts{}; ssl_opts.pem_file(\u0026quot;client.pem\u0026quot;); mongocxx::options::client client_opts{}; client_opts.ssl_opts(ssl_opts); auto client = mongocxx::client{uri{\u0026quot;mongodb://host1/?..\u0026lt;other_parameters\u0026gt;..\u0026amp;ssl=true\u0026quot;}, client_opts};  Replicaset host-list syntax The MongoDB connection URIs above could be syntactically valid as HTTP URLs if we simply replace \u0026quot;mongodb\u0026quot; with \u0026quot;http\u0026quot;, but it isn't always so. There is one key HTTP-like rule-breaker which is necessitated by the fact that MongoDB typically uses replica sets. If a replica set has hosts hostA, hostB and hostC, each using the same port 27017, then the URI can accept them in a comma-separated list:\nmongodb://akira:secret@hostA:27017,hostB:27017,hostC:27017/?ssl=true\u0026amp;minPoolSize=50\nIf you only specify one host (let's say hostA:27017) that the driver can connect to it will automatically query for the full replica set configuration/status. After that it will be aware of all the other hosts, so it might seem unnecessary to specify multiple hosts to connect to. But imagine a time when hostA was shut down for maintenance, or had crashed, and the client has to establish a new connection.\nDefaults Most of of the URI parameters have default values.\nMost useful to know is that the hostname default is localhost, and the port default is 27017. It's these that make it possible for clients to connect without specifying any connection options at all. All you need to do is to set up a mongod or mongos process running on port 27017 with no user authentication on the same server as the client.\n"
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/rw_data_guide/bson_format/",
	"title": "The data: BSON documents",
	"tags": [],
	"description": "The JSON-like data format used in MongoDB",
	"content": " MongoDB stores both user collections' data and also its system collections' data in one and only one binary format - the BSON format. It is a binary format for serializing objects of arbitrary keys and values, the same as JSON.\n   JSON equivalent Serialized form in hexadecimal byte values     \u0026nbsp; 0x3e = 62 (byte size of this document) as int32\n3e 00 00 00   { \u0026nbsp;   \u0026nbsp; \u0026quot;_id\u0026quot; : 7, (datatype 0x01 = double), (cstring \u0026quot;_id\\0\u0026quot;), (7 as a double floating-point value)\n01 \u0026nbsp; 5f 69 64 00 \u0026nbsp; 00 00 00 00 00 00 1c 40   \u0026nbsp; \u0026quot;instr\u0026quot; : \u0026quot;XYZ 3m\u0026quot;, (datatype 0x07 = string), (cstring \u0026quot;instr\\0\u0026quot;), ((not-cstring!) string len 0x07 in int32), (byte array \u0026quot;XYZ 3m\u0026quot; (6 bytes) + an extra null byte)\n02 \u0026nbsp; 69 6e 73 74 72 \u0026nbsp; 00 \u0026nbsp; 07 00 00 00 \u0026nbsp; 58 59 5a 20 33 6d 00   \u0026nbsp; \u0026quot;hval\u0026quot; : 904.72, (datatype 0x01 = double), (cstring \u0026quot;hval\\0\u0026quot;), (904.72 in a double)\n01 \u0026nbsp; 68 76 61 6c 00 \u0026nbsp; f6 28 5c 8f c2 45 8c 40   \u0026nbsp; \u0026quot;ts\u0026quot; : ISODate(\u0026quot;2019-07-21T01:12:15.348Z\u0026quot;) (type 0x09 UTC datetime), (cstring \u0026quot;ts\\0\u0026quot;), (UTC milliseconds since the Unix epoch in an int64)\n09 \u0026nbsp; 74 73 00 \u0026nbsp; f4 1e 16 12 6c 01 00 00   } \u0026nbsp;   \u0026nbsp; 0x00 datatype indicator for a BSON document\nN.b. the document type is the only one that has it's type indicator at the end.      Notes on the above example    The \u0026quot;cstring\u0026quot; type used for key names means UTF-8 character string with a tailing null byte on the end. This means all valid UTF-8 strings except the case of having a string of one or more null bytes, which is pointless as a key name value anyhow. Even though the \u0026quot;_id\u0026quot; value above looked like an integer to our naked eye, mongo javascript shell made it a 64 bit double float. This happened because I created the document in the (javascript) mongo shell. Depending on your client language you may or not have a floating-point or integer-style value by default; in all client languages you can specify the type if you are more explicity. E.g. in the mongo javascript shell I could have created the _id value as new NumberInt(7), or NumberLong. You probably worked this out: Little-endian byte order is used for all the basic types. I.e. everything that isn't a byte string/array.      shell example with bsondump of a single-doc collection   ~$ mongodump \u0026lt;connection_args_to_default_test_db\u0026gt; --eval 'db.foo.insert({\u0026quot;_id\u0026quot;: 7, \u0026quot;instr\u0026quot;: \u0026quot;XYZ 3m\u0026quot;, \u0026quot;hval\u0026quot;: 904.72, \u0026quot;ts\u0026quot;: new ISODate()})' WriteResult({ \u0026quot;nInserted\u0026quot; : 1 }) testrs:PRIMARY\u0026gt; bye ~$ mongodump \u0026lt;connection_args\u0026gt; -d test -c foo --out /tmp/dump 2019-07-21T10:13:54.137+0900\twriting test.foo to 2019-07-21T10:13:54.149+0900\tdone dumping test.foo (1 document) ~$ ~$ bsondump /tmp/dump/test/foo.bson {\u0026quot;_id\u0026quot;:7.0,\u0026quot;instr\u0026quot;:\u0026quot;XYZ 3m\u0026quot;,\u0026quot;hval\u0026quot;:904.72,\u0026quot;ts\u0026quot;:{\u0026quot;$date\u0026quot;:\u0026quot;2019-07-21T01:12:15.348Z\u0026quot;}} ~$ #bsondump's job is to print a JSON string representation. To make strictly ~$ # valid JSON it must use only the base JSON scalar datatypes. To do this it ~$ # follows MongoDB's \u0026quot;Extended JSON\u0026quot; rules such as using {\u0026quot;$date\u0026quot;:\u0026quot;...\u0026quot;} for ~$ # an ISODate. This makes it look like \u0026quot;ts\u0026quot; was a nested object, but it was ~$ # scalar value - the BSON spec's 64-bit integer of UTC milliseconds. ~$ ~$ od -An -t x1 /tmp/dump/test/foo.bson 3e 00 00 00 01 5f 69 64 00 00 00 00 00 00 00 1c 40 02 69 6e 73 74 72 00 07 00 00 00 58 59 5a 20 33 6d 00 01 68 76 61 6c 00 f6 28 5c 8f c2 45 8c 40 09 74 73 00 f4 1e 16 12 6c 01 00 00 00 ~$ ~$ #The 0x3e (= 62) little-endian int32 at the beginning is the byte length of ~$ # the first doc. There is no size field/value for the whole collection at ~$ # the beginning of the dump file.    The fine detail above is just being shown for the curious. If you want to see even more check out the specification page in bsonspec.org.\nThe main point for database users is:\n BSON is a variable-length format that packs the key-value pairs one after the other in tuples of type + key name [+ byte length when not a fixed-length type] + value data.  For brevity no nested objects or arrays were shown in the example above, but they are also merely another key-value tuple in this serialization/deserialization algorithm.  Key names consume space in every document, even when all the documents in a collection have exactly the same ones. Use short key names to save space. The format places no expectations/assumptions about which fields are included, or in which order they are serialized.  Exception: arrays are implemented as objects that must have keys 0, 1, 2, ...   In practice BSON is the encoding of MongoDB and isn't used in any other software as popular as MongoDB yet. Nonetheless the BSON specification is one thing and MongoDB is another. There are some extra requirements that MongoDB places on any BSON object is will store:\n 16MB maximum size. The BSON specification places no upper limit on the size of data it encodes but the MongoDB database server and the drivers do. \u0026quot;_id\u0026quot; field: Every document saved to a collection will have an \u0026quot;_id\u0026quot; field value. It is the primary key value of all collections. One of an ObjectId() type will be given automatically if none is specified at insert time.  Datatypes JSON only supports the same datatypes that a Javascript tokenizer will handle. (If you are Javascript programmer you are no doubt thinking 'But what about other types such as Date?' Surprise- these are not covered by the JSON specification.)\n String (Null-terminated UTF-8) Number (JSON does not specify the binary format) Boolean Null Object Array  BSON extends to have these necessary datatypes that mostly any database would need:\nNumber types:\n int32 int64 uint64 Double (8-byte IEEE 754-2008 format) Decimal (16-byte IEEE 754-2008 format) Datetime (without timezone, i.e. assumed to be UTC always) Timestamp Generic binary data ObjectID (MongoDB uses this type. It would be called a GUID in some other databases that exist.)  BSON also include these (in my opinion) exotic-for-a-database-system datatypes\n Min key Max key Javascript code (As a UTF8 string, i.e. not in a compiled, runtime-executable format.) A few special 'binary types' Function (as a compiled, runtime-executable format????) UUID MD5  "
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/connecting_guide/cs_nw_details/drivers/",
	"title": "Drivers and &#39;the wire&#39;",
	"tags": [],
	"description": "Putting perspective on what a MongoDB driver is through a description of the shape and behaviour of your client requests (and server replies) as TCP traffic",
	"content": " From the perspective of the driver developer a MongoDB driver:\n Marshalls data from the language's native types into the format the MongoDB server requires (== a BSON payload proceeded by a few classically simple network fields in each packet's header area). Sends and receives that info, keeping track of which reply from a server matches which request.  When using a connection pool it also keeps a track of which thread the requests were sent from  Goes to error handling when there are network interruptions like abrupt socket closure and other TCP casuality situations Implements a lot of detail in the 'Meta' API driver specification for server discovery and monitoring (\u0026quot;SDAM\u0026quot;) and server selection.  From the application developer's perspective the MongoDB driver:\n Presents the database as an object you can push data in and pull data out of. The API provided is idiomatic for your language. E.g. where Java programmers run a find() method on a collection object, C driver users run a mongoc_collection_find() function that takes a mongoc_collection_t* pointer argument, etc.  To look at it from another side this is what the driver API doesn't do:\n Involve the application programmer in maintaining the TCP socket connections. Involve the application programmer in determining which remote servers are the current primaries (i.e. the one that the writes happen on first) Expose network packet data in the wire protocol format  Apart from the fact that you open a connection, and there can be exceptions thrown when a server crashes or the network is disconnected, there is limited expression in the API that the database is on a remote server. There are no network-conscious concepts the user must engage with such as 'queue this request', 'pop reply off incoming message stack', etc.\nMany drivers; one Wire Protocol Regardless of which driver you are using, at the Wire Protocol layer they are all the same fundamentally. If they are contemporary versions there's a good chance the BSON payload in each Wire protocol packet is identical excluding ephemeral fields like a timestamps.\nThe format of data in MongoDB Wire Protocol requests and responses is relatively simple, but it is a binary one and is far from being human-readable. The below comes from TCP payloads captured using tcpdump, manually unwrapped using command line tools od and bsondump according to the info in the MongoDB wire protocol documentation.\n   Example find in various APIs  MongoDB wire packet  mongod code     mongo shell db.foo.find({\u0026quot;x\u0026quot;: 99};\nPyMongo db.foo.find({\u0026quot;x\u0026quot;: 99})\nJava db.getCollection(\u0026quot;foo\u0026quot;).find(eq(\u0026quot;x\u0026quot;, 99))\nPHP $db-\u0026gt;foo-\u0026gt;find(['x' =\u0026gt; 99]);\nRuby client[:foo].find(x: 99) → OP_MSG\nlength=180;requestID=0x1b73a9;responseTo=0;opCode=2013(=OP_MSG type)\nflags=0x00.0x00\nsection 1/1 = {\n\u0026nbsp; \u0026quot;find\u0026quot;:\u0026quot;foo\u0026quot;,\n\u0026nbsp; \u0026quot;filter\u0026quot;:{\u0026quot;x\u0026quot;:99.0},\n\u0026nbsp; \u0026quot;$clusterTime\u0026quot;:{ ... }},\n\u0026nbsp; \u0026quot;signature\u0026quot;:{ ... },\n\u0026nbsp; \u0026quot;$db\u0026quot;:\u0026quot;test\u0026quot;\n} → mongo::FindCmd::run   (A cursor object with first batch results) ← OP_MSG (as a reply)\nlength=180;requestID=0xb5a;responseTo=0x1b73a9;opCode=2013(=OP_MSG type)\nflags=0x00.0x00\nsection 1/1 = {\n\u0026nbsp; \u0026quot;cursor\u0026quot;:{\n\u0026nbsp; \u0026nbsp; \u0026quot;id\u0026quot;:{\u0026quot;$numberLong\u0026quot;:\u0026quot;0\u0026quot;},\n\u0026nbsp; \u0026nbsp; \u0026quot;ns\u0026quot;:\u0026quot;test.foo\u0026quot;,\n\u0026nbsp; \u0026nbsp; \u0026quot;firstBatch\u0026quot;:[\n\u0026nbsp; \u0026nbsp; {\u0026quot;_id\u0026quot;:ObjectId(\u0026quot;5b3433ad88d64ee7afb5dc80\u0026quot;), \u0026quot;x\u0026quot;:99.0,\u0026quot;order_cust_id\u0026quot;:\u0026quot;AF4R2109\u0026quot;}\n\u0026nbsp; ]\n\u0026nbsp; },\n\u0026nbsp; \u0026quot;ok\u0026quot;:1.0,\n\u0026nbsp; \u0026quot;operationTime\u0026quot;:{ ... },\n\u0026nbsp; \u0026quot;$clusterTime\u0026quot;:{ ... },\n\u0026nbsp; \u0026quot;signature\u0026quot;:{ ... },\n\u0026nbsp; \u0026quot;keyId\u0026quot;:{\u0026quot;$numberLong\u0026quot;:\u0026quot;0\u0026quot;}}}\n} ← ↲    OP_QUERY and early generations An optional detour for those who knew the original Wire protocol messages (OP_QUERY, OP_INSERT, etc.) and are interested in what traffic looked like with these.\n  Expand me...   The above is latest-and-greatest OP_MSG format. At time of writing only the 3.6+ mongo shell and dev-branch drivers would be using it. In truth most driver versions are still being shoe-horned into the legacy OP_QUERY message type.\nPer its name OP_QUERY was meant to only be for queries, but was repurposed for mostly any type of request message. In its network packet fields it included a \u0026quot;fullCollectionName\u0026quot; field because queries always need a a db and collection name scope). But there are commands that don't have a collection scope (eg. replicaSetGetStatus, createUser) but don't have a dedicated wire protocol message type either. How to send them? The workaround for those cases was that \u0026quot;$cmd\u0026quot; was used as a dummy collection name at the end of the \u0026quot;fullCollectionName\u0026quot; field. This workaround became so standard that it is even set this way for commands such as find which do need a collection scope. You can see in the example below that the collection name \u0026quot;foo\u0026quot; has moved inside the BSON and is absent outside.\n   Legacy wire packet examples     OP_QUERY\nlength=215;requestId=0x6633483;responseTo=0;opCode=2004(=OP_QUERY type)\nfullCollectionName=\u0026quot;test.$cmd\u0026quot; //N.b. the dummy \u0026quot;$cmd\u0026quot; collection name numberToSkip=0;numberToReturn=0xffff\ndocument = {\n\u0026nbsp; \u0026quot;find\u0026quot;:\u0026quot;foo\u0026quot;,\n\u0026nbsp; \u0026quot;filter\u0026quot;:{\u0026quot;x\u0026quot;:99},\n\u0026nbsp; \u0026quot;lsid\u0026quot;:{ ... },\n\u0026nbsp; \u0026quot;$clusterTime\u0026quot;:{ ... },\n\u0026nbsp; \u0026quot;signature\u0026quot;:{ ... },\n\u0026nbsp; \u0026quot;keyId\u0026quot;:{\u0026quot;$numberLong\u0026quot;:\u0026quot;0\u0026quot;}}}\n}   OP_REPLY\nlength=301;requestId=0xbb8;responseTo=0x6633483;opCode=1(=OP_REPLY type)\nresponseFlags=0x08(=AwaitCapable)\ncursorID=0 //important for getMore cmds that follow, if any;\nstartingFrom=0;numberReturned=1\ndocument = {\n\u0026nbsp; \u0026quot;cursor\u0026quot;\n\u0026nbsp; {\n\u0026nbsp; \u0026nbsp; \u0026quot;firstBatch\u0026quot;:[\n\u0026nbsp; \u0026nbsp; \u0026nbsp; {\u0026quot;_id\u0026quot;:ObjectId(\u0026quot;5b3433ad88d64ee7afb5dc80\u0026quot;), \u0026quot;x\u0026quot;:99.0,\u0026quot;order_cust_id\u0026quot;:\u0026quot;AF4R2109\u0026quot;}\n\u0026nbsp; \u0026nbsp; ],\n\u0026nbsp; \u0026nbsp; \u0026quot;id\u0026quot;:{\u0026quot;$numberLong\u0026quot;:\u0026quot;0\u0026quot;},\n\u0026nbsp; \u0026nbsp; \u0026quot;ns\u0026quot;:\u0026quot;test.foo\u0026quot;},\n\u0026nbsp; \u0026nbsp; \u0026quot;ok\u0026quot;:1.0,\n\u0026nbsp; \u0026nbsp; \u0026quot;operationTime\u0026quot;:{ ... },\n\u0026nbsp; \u0026nbsp; \u0026quot;signature\u0026quot;:{ .. }\n\u0026nbsp; }\n}    My way of looking at is:\n In the beginning there were just collection editing or reading commands (query, insert, update, delete) and four wire packet types for those, plus a reply message type. The db+collection namespace was put in a network field, outside the BSON payload document. Soon there many more command types that the database server accepted. A generic command wire packet format was needed. The existing drivers (that needed to be supported for some time) started using OP_QUERY overloaded for this purpose. A generic command wire packet type OP_COMMAND was invented! And used by mongo shell v3.4(?) and between nodes in clusters and replica sets. But it didn't go mainstream. Instead the OP_MSG type has become the new standard, to be used by 4.2? era drivers. Neither the collection name or database name is in the network header fields - they'll be in \u0026quot;ns\u0026quot; (namespace) inside the BSON payload instead.    Database command type You might have noticed that there's no primary / headlined / specially labeled value in the BSON command object that indicates what sort of command the client is sending.\nYou might be wondering 'Does the server run through a list of key-value pairs in fixed order until it gets a match?' (E.g. if (commandMessage.hasKey(\u0026quot;find\u0026quot;) then --\u0026gt; FindCmd:run(), else if commandMessage.hasKey(\u0026quot;update\u0026quot;) -\u0026gt; UpdateCmd::run(), etc. ....?).\nNope, a simpler mechanism is used. From util/net/op_msg.h:\nStringData getCommandName() const { return body.firstElementFieldName(); }  Take the key name from the first key-value pair. End of function.\nA lesson from this is that order in BSON can matter (at least to MongoDB). Important for driver developers, but not application programmers as the driver API will take care of this point for you.\nWhat it looks like to the programmer I don't want to re-invent the documentation wheel for this part. MongoDB's official documentation tutorials are good and cover many language samples in one page. Some links for a couple of types of operations:\n Document insert example (Python, Java, Node.js, PHP, C#, Perl, Ruby, Scala) Query example (Python, Java, Node.js, PHP, C#, Perl, Ruby, Scala)  "
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/connecting_guide/human_ix/mongo_shell/",
	"title": "The mongo shell",
	"tags": [],
	"description": "The simple truth about the mongo shell",
	"content": " What it is The mongo shell is just another client application. There, I said it.\nIt is not a special node within a MongoDB replica set or cluster. It is an application that connects and communicates with the mongod (or mongos) nodes with the same MongoDB Wire protocol TCP traffic that any other application could. If it was a black box rather than being open source, you could reverse-engineer it even without super-dooper elite hacker skills. It has no special sauce that gives it elevated privilege or better performance compared to what any MongoDB driver-using application can have.\nWhat is unique about the mongo shell compared to the thousands of other MongoDB-connected applications you might install on your computer is that is an interactive CLI (command line interpreter) a.k.a. REPL (read-evaluate-print loop). It's not the only MongoDB CLI that has ever existed, but it is the only popular one to date.\nWhy use it Having an interactive shell is a practical requirement for doing administration, so basically everyone will use it for that reason at least. Most people will also use it for learning. The MongoDB documentation uses mongo shell syntax all over too.\nConnection examples On the unix (or windows) shell you can specify connection options, and optionally input (a script file to run or a single string to run).\nIf you are not already familiar with the command-line arguments the mongo shell accepts please expand the following section.\n  Expand me to see mongo shell connection examples   The examples beneath show how to connect to:\n A replicaset named \u0026quot;merch_backend_rs\u0026quot; It has two normal, data-bearing nodes running at  dbsvrhost1:27017 (the current primary), dbsvrhost2:27017 (currently a secondary),  And an arbiter on a third host somewhere. The main user database is \u0026quot;orderhist\u0026quot;. There is a user \u0026quot;akira\u0026quot; with password \u0026quot;secret\u0026quot;, and the usual \u0026quot;admin\u0026quot; db is the authentication database (i.e. where the system.users and related system collections are).  Common usage forms shown below. See here for the all the options.\n# Most typical mongo --host dbsvrhost1:27017/orderhist -u akira -p secret --authenticationDatabase admin # Specify the replicaset name to guarantee a proper replset connection mongo --host merch_backend_rs/dbsvrhost1:27017,dbsvrhost2:27017/orderhist -u akira -p secret --authenticationDatabase admin # Using a mongodb URI connection string, the same as in your application code. mongo --host 'mongodb://akira:secret@dbsvrhost1:27017,dbsvrhost2:27017/orderhist?authSource=admin\u0026amp;replicaSet=merch_backend_rs' # If you have disabled authentication in the mongod configuration, and it is # running on port 27017 on localhost, and you want to use the \u0026quot;test\u0026quot; db ... # Bingo!, the naked command will work. mongo # Execute a javascript script file mongo --host dbsvrhost1:27017/orderhist -u akira -p secret --authenticationDatabase admin daily_report.js # Execute a javascript statement as a command-line argument. mongo --host dbsvrhost1:27017/orderhist_db -u akira -p secret --authenticationDatabase admin --eval 'var acnt = db.collection_a.count(); var bcnt = db.collection_b.count(); if (acnt != bcnt) print(\u0026quot;Reconcilliation error: Collection a and b counts differ by \u0026quot; + Math.abs(acnt - bcnt));'  In the case of sharded cluster do not add a replicaset parameter in the connection arguments. Just provide the hostname and port of the mongos node you are connecting to (or a comma-delimited list of them).\n  Internals Although it is made with C++ the language that this CLI interprets is Javascript. Apart from a very small number of legacy, imperative-style command expressions such as \u0026quot;show databases\u0026quot;, \u0026quot;exit\u0026quot;, etc. everything is Javascript.\nShell parsing The syntax exception: Legacy MySQL-like commands use \u0026lt;database_name\u0026gt; show databases show collections  Apart from \u0026quot;use database_name\u0026quot;, which sets the database namespace the client sends in the Wire Protocol requests, these legacy command expressions are all translated internally to a Javascript function. For example \u0026quot;show collections\u0026quot; is really:\n//The real code behind \u0026quot;show collections\u0026quot;: if (what == \u0026quot;collections\u0026quot; || what == \u0026quot;tables\u0026quot;) { db.getCollectionNames().forEach(function(x) { print(x); }); return \u0026quot;\u0026quot;; }  To see how these parsing exceptions are achieved you can look at the shellHelper.show function in mongo/shell/utils.js.\nPlain Javascript The mongo shell will process javascript with referring to any database context, if you want to. Below are some client side-only expressions and functions, pretty much identical to those you can do in the native Javascript supported in web browsers etc.\nvar x = 1; for (i = 0; i \u0026lt; 100; i++) { print(i); } function max(a, b) { return a \u0026gt; b ? a : b; }  Javascript that acts with database connection objects Unless you use the --no-db argument there will be the \u0026quot;db\u0026quot; special global object which can be used to send db command messages over the connection to a MongoDB server.\nuse \u0026lt;database_name\u0026gt; //set current database namespace db.version() //database namespace doesn't affect this particular command //Because I did not capture the result into a variable (i.e. I didn't put \u0026quot;var version_result = …\u0026quot; at the front) // the shell will capture the return value from db.getVersion() and auto-print it here 3.4.4 db.serverStatus() ///database namespace doesn't affect this particular command //As before, the return value from the statement will be auto-printed. db.serverStatus() { \u0026quot;host\u0026quot; : \u0026quot;myhost.mydomain\u0026quot;, \u0026quot;version\u0026quot; : \u0026quot;3.4.4\u0026quot;, \u0026quot;process\u0026quot; : \u0026quot;mongod\u0026quot;, \u0026quot;pid\u0026quot; : NumberLong(2175), ... ... \u0026quot;ok\u0026quot; : 1 } var cursor = db.\u0026lt;collection_name\u0026gt;.find({\u0026quot;customer_id\u0026quot;: 10034}); //this command is affected by the database namespace while (cursor.hasNext()) { var doc = cursor.next(); printjson(doc); } ...  In the example above:\n \u0026lt;database_name\u0026gt; is set as the db scope. This will go in command objects put into MongoDB Wire protocol messages sent from here. It won't be changed unless there is another \u0026quot;use xxxxx\u0026quot; statement or something that implies it, like a db.getSiblingDB(...) function. db.getVersion() will create a buildinfo command as BSON object. Through javascript-interpreter-to-C++-code boundary and then the C++ driver library that is put that in wire protocol message message and send it the db server. The response travels those layers in reverse, finally ending with the buildinfo result in Javascript object, from which the version property is picked and printed. db.serverStatus() is a helper function that executes _db.adminCommand({serverStatus: 1})) instead. I.e. this time the BSON object being packed and set is {serverStatus: 1} compared to {hostinfo: 1}. At the return the whole object (rather than just one scalar value property) is pretty-printed onto the terminal output. A similar pattern at first to the last two commands, with a {find: \u0026quot;database_name.collection_name\u0026quot;} BSON object being sent first. The result will contain the found docs, at least if they number 100 or less and fit within the max wire protocol message size. In that simple case it is one request, one response, end. But in the case not all of the documents are delived in one go the result will also contain a (cursor) \u0026quot;exhaust\u0026quot;: false value and cursor id value. The driver automatically continues fetching more results from server-side (assuming you bother to iterate the fetched first batch to its end) with a different type of command \u0026ndash; the getMore command (or in the legacy way, an OP_GET_MORE wire protocol message).  Ever-present db namespace The sent commands always includes a database namespace. You can change it at will (\u0026quot;use another_db_name\u0026quot;) so it is variable, but it can't be empty/null. Default is \u0026quot;test\u0026quot;.\nSome commands don't logically require a db namespace \u0026ndash; eg. isMaster, addShard, replSetGetStatus \u0026ndash; but they won't work unless it is set to \u0026quot;admin\u0026quot;. Many a time I've had those fail until I typed \u0026quot;use admin\u0026quot; and tried again. Some like isMaster you don't notice because you're probably never call it except by the a shell helper function (db.isMaster()) that sets it.\nCrystal ball gazing: Having said all this it isn't out the question that what is unnecessary will be removed in the future. The OP_MSG message format in particular doesn't require or even permit a db namespace in the network fields, so once older messages formats stop being supported some rationalization is possible. It will be interesting to see if the code base can handle having this trimmed out.\nExplicit db connection objects You don't have to use the \u0026quot;db\u0026quot; global var if you don't want to. You can manually create other live MongoDB connections objects with connect(\u0026lt;conn_uri\u0026gt;), or new Mongo(\u0026lt;conn_uri\u0026gt;) and give those whatever variable name you like. It would be an untypical way to use the mongo shell however.\nRecap To recap the mongo shell:\n Uses the MongoDB wire protocol to communicate with MongoDB servers the same as any application It is C++ internally Makes use of a javascript engine library and \u0026quot;readline\u0026quot;-style line editor library to provide a live Javascript command line interpreter / REPL. It doesn't handle the wire protocol 'raw' or control TCP primitives itself. It uses the standard C++ MongoDB client driver for that. Can be used to run Javascript code for the sake of Javascript alone, but the purpose is communicate with the database There is one \u0026quot;db\u0026quot; MongoDB connection object created which represents the connection to the standalone mongod or replicaset of mongod nodes or mongos host you specified with the --host argument when you began the shell. The behind-the-scenes flow every time you execute a db.XXX() command:  You create documents as Javascript objects, and execute Javascript functions in the interpreter. The mongo shell converts the Javascript objects to BSON, and the functions to known MongoDB server commands, which are also serialized in a BSON format. These include the argument values (if any), puts it into the OP_MSG request (or legacy OP_QUERY or the v3.2 experimental OP_COMMAND format requests) and sends it over the network The server responds with a reply in BSON The mongo shell shunts the BSON into the TCP payload to C++ object, then cast/marshalled to a javascript object through the Javscript engine. The converted-to-Javascript-binary-format result is assigned into a javascript variable if you set one, or auto-printed into the shell terminal if you did not.   Q. \u0026quot;But what about server-side Javascript? That's what MongoDB uses right?\u0026quot;\nNo, that's not what MongoDB uses. Well it can interpret and execute some javascript functions you send to it, but they're only for running within:\n a MapReduce command, or (Superseded by $expr in v3.6; removed v4.2): if using a $where operator in a find command, or (Deprecated v3.4; removed v4.2): as the \u0026quot;reduce\u0026quot;, \u0026quot;keyf\u0026quot; or \u0026quot;finalize\u0026quot; arguments in a group command.  These functions are javascript, but they get packed inside a special BSON datatype (just a string with a different enum value for type) to be sent to the server and the mongod is the only program I know that has ever been programmed to unpack that format. Being javascript it is a lot slower than the native C++ processing in the mongod process.\n"
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/connecting_guide/cs_nw_details/ha_connections/",
	"title": "Automatic failover",
	"tags": [],
	"description": "A light introduction regarding how MongoDB drivers will automatically respond to server failures",
	"content": "Although it hasn't been discussed in the book in detail so far, in the typical situation your client will be connected to a replica set, that is it will be sending writes to one node (the Primary) whilst also being connected to other nodes that are Secondaries. As long as the Primary is alive, accepting commands, and the other nodes continue to agree that it is the primary, that will continue without change.\nBut if the original primary node dies, or get cuts off network-wise from the other nodes, they will initiate a replica set election and one of them will become the new primary.\nThere be may some writes to the original primary that will have to be rolled back, particularly in an unbalanced network partition situation. That is an issue that means the rolled back operations will have to be examined manually and re-applied manually by administrators after the incident.\nThe election would take a fraction of a second usually (2 seconds at most), faster than any human is going to be able to react, so you want the application to automatically switch to use the new primary.\nQ. What do you program to make sure the client application send requests to the new primary, and avoid having every single thread in every single process throw endless repeating 'socket exception' or 'write rejected not primary' errors?\nA. Nothing. Because all the drivers have replica set automatic failover logic programmed into them you can't program (and don't need to program) anything extra to achieve re-routing to the surviving nodes.\nFrom the point of view of the application code using a MongoDB connection, database, or collection object it can continue to use the same object. The driver will route the reads and writes to the new primary. If you have explicitly requested reads to come from a non-primary node (possible with a feature called readPreference, to be discussed later) the connection for those will be changed away from the new primary node too.\nSo the challenge of staying connected is solved for you out of the box whichever driver you use.\nWhat is not handled is what to do with rolled back data. Rollbacks can occur in the case that the original primary doesn't step down immediately, and accepts some writes in the brief time before it determines it has lost the status updates from the other nodes and steps itself down. This will be explained more in the \u0026quot;Making MongoDB stable\u0026quot; chapter.\n"
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/connecting_guide/human_ix/",
	"title": "How to connect from your app or a terminal",
	"tags": [],
	"description": "First things first: how you connect to the database service as a client app or administrator",
	"content": "  Connection string URI  MongoDB connection URI syntax\n The mongo shell  The simple truth about the mongo shell\n "
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/guide_intro/",
	"title": "What you can do with MongoDB",
	"tags": [],
	"description": "A dash through the top reasons to use MongoDB",
	"content": " Develop faster Mostly everyone who has used one of the 'NoSQL' databases which came into popularity post-2010, of which MongoDB is the most popular, already appreciate this. But for those of you still using relational databases the key reasons for the higher development productivity are:\n Reduced lines of code You don't need to perform a full mental switch between the paradigm of your application's data structure v.s. the server-side data storage mechanism.  You are a glucose-powered computer and context change is expensive. Only so much glucose can be converted by your neurons into thought-sparks a day, so save your sparks for things that are more valuable to you.\nBroadest client language support Want to do things quicker by using more developers / tools? For example your Web front-end has been created in language X, but the data scientists that will mine gold from the big data need to use language Y? MongoDB has got you covered - C, C++, C#, Erlang, Go, Java, Node.js, Perl, PHP, Python, Ruby and Scala are supported.\nThe simplicity of the client API will also reduce the technical debt of your codebase significantly. To the original developer this may seem unimportant, but for the development department that owns the technology it is more important than the speed of new feature development.\nWrite and read data faster This has always been my favourite feature of MongoDB.\nLet's take a trip back in time, to 2010 or so. The first MongoDB version I was using was 1.6, and I had a task to load a dataset of some 10's of GB. I can't remember the client program, but it might have been mongoimport.\nAs the data load progressed:\n I was pleasantly surprised by the rate of document inserts. I was a little shocked by the rate of inserts. I was getting suspicious of the insertion rate. Was the counter just a client-side lie? (No, I confirmed server-side.) I started sanity-checking the numbers, by datasize too. I arrived at a figure ~30 MB/s. 30 MB/s reminded me of something. At the time commodity HDD specs advertised 30MB/s write speeds. I checked the server's disk specs. They were the same 30 MB/s rate.  The takeaway for me was the hardware specs were no longer some sort of fantasy numbers way above the user's reality, which was a given for RDBMS-using application developers until then. If the hardware manufacturers have engineered their equipment to flick x million or billion electrons per second from one silicon nano-suburb to another, I could now use all those electrons for my data purposes.\nWith the change in hardware-land to SSDs, and furthermore MongoDB's change to the WiredTiger storage engine, MongoDB users now enjoy throughput and latency somewhat higher than the puny 30MB/s figure above. But the key point is to expect MongoDB to redline your hardware's throughput and/or latency by the volume of your data being moved, without the database software eating a noticeable chunk of the server capacity for itself.\nGo big The growth of e-commerce and social networking in the noughties lead to many thousands of businesses having a problem that was limited to few before. This was dataset sizes that exceeded the capacity of the biggest server you could afford to buy. If your user base was large, well, you either became one of the few companies that engineered around this by getting good at distributed data in your server application, or you limited data detail and panicked about whether you'd last the next 2 months before the server RAM and disk upgrades were delivered.\nThe NoSQL databases for the most part came with a huge benefit for this businesses / websites - easy data partitioning. You don't program the distribution logic in your application, instead you leave it to the database driver or the db server node you connect to.\nThe NoSQL databases mostly all include replication, making automatic database failover another thing that happens on the other 'side' of the database driver.\nWhich field(s) should be chosen to partition data is still a very important decision that you need to make for yourself, but after that your MongoDB cluster will allow you to grow your data up to (Single server storage size) x (100's).\nGet bigger quickly Starting with a single, unsharded MongoDB replica set is not a problem if suddenly you find you data volume growing. A single replica set can be dynamically converted, in configuration, to being the first shard of a one-shard MongoDB cluster. With no downtime or any reinsertion of your user data you can gain the ability to add new shards. Add one, two, as many as are needed, and the first cluster's data will be redistributed automatically until the number of documents in collections is balanced between the shards.\nThis has no impact on the logical view of the data to the client. To the client it is as though there is one server with larger capacity. Even document data that might happen to be in the process of being moved from one shard to another as part of shard balancing will not experience an error or delay. (It will delay the background move instead, if the access is write).\nBe small You can also be small - MongoDB does not hang some performance-lowering burden of distributed data management on your database server, or place configuration burden on you as a database administrator, if you aren't using it.\nYou don't even have to have a replica set if, in a rare sort of use case, you can afford for your database to be down (say if a data center's power goes out) and furthermore don't care if the data is lost (say if the hard disk suffers irreversible corruption). MongoDB can run as a standalone mongod process and this lets you forgo the cost of having a second or third server. (Starting from v4.0 technically all nodes must be in a replica set by configuration, but that can be single-node replica set.)\nGet smaller quickly I'm just kidding. But if you weren't, yes, you can use remove (== document delete), drop (== whole collection drop), rs.remove(\u0026lt;replica member\u0026gt;) and removeShard.\nSurvive server, data center failures MongoDB replica set members contain copies of each other's data to within whatever limit of time it takes for an update on the primary to be replicated to the secondaries. This can be just a millisecond in the better cases.\nAn important corollary to this is: the drivers (i.e. the MongoDB API library you use in your code to connect to MongoDB) are all replicaset-aware, regardless of which language you are version.\nIf you are using sharding, you will connect to a mongos node. The mongos node does the failover handling in that case.\nSo if a server dies:\n The remaining replica set nodes will notice this (default is with \u0026lt;= 2 seconds, the 'heartbeat' cycle). If the server that died was the primary, they will hold a new election and one of the prior secondaries becomes the new primary. There is a dynamic replicaset state shared between them which is updated. When the former primary is restarted it will receive and act accordingly to that new state where it is a secondary, not try to continue as it was before it's halt. The clients that were connected, with the replica-set aware driver code, will detect the failure of the read/write they were performing at the time. Apart from the 'blip' of reads and writes that failed because they were en-route to the former primary just about the time it died, the application's database functionality remains up. E.g. if you were serving 10,000 web page request a minute some, say dozens, will have database errors. The web pages served before and after will be unaffected. You don't have to accept that the client fails to perform it's intended logic during the period of time between the first primary's crash and the new one stepping up. The type of error returned in the event of a lost primary will allow you to recognize that a new primary will shortly step up, and that you can try to do the same thing again (say with try-catch block). MongoDB drivers do not automatically retry for a good reason though - whether something should be retried or not depends on your application's requirements. So it is left to programmers to explicity choose what to do - retry, or not. Redo writes assuming the original context is still valid, or not. Just try the write again, and if there is a duplicate error you could assume the write originally succeeded (and was replicated to the secondary node that subsequently became the new primary)  Know the inside out Diagnostic information MongoDB includes diagnostic information.\n Log files db.serverStatus() Configuration:  db.cmdLineOptions() rs.status(), rs.conf() sh.status() The sharding config db.  currentOp and system.profile sampling documents  These are evidence you can examine to learn a lot about the state of your MongoDB instances.\nGraphical tools are not included in the normal server and client installation packages, but you can find them in MongoDB's cloud utlities, or third-party metric monitoring tools.\nSource code A nice feature, for the C++ programmers especially, is that the source code (excluding enterprise modules such as LDAP and Kerberos authentication, auditing, etc.) is publicly available.\n"
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/connecting_guide/cs_nw_details/",
	"title": "Client-Server network communication",
	"tags": [],
	"description": "The shape of network communication between MongoDB clients and server",
	"content": "  Drivers and \u0026#39;the wire\u0026#39;  Putting perspective on what a MongoDB driver is through a description of the shape and behaviour of your client requests (and server replies) as TCP traffic\n Automatic failover  A light introduction regarding how MongoDB drivers will automatically respond to server failures\n "
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/connecting_guide/",
	"title": "Connecting to MongoDB",
	"tags": [],
	"description": " ",
	"content": " Chapter 2 Connecting to MongoDB  How to connect from your app or a terminal  First things first: how you connect to the database service as a client app or administrator\n Client-Server network communication  The shape of network communication between MongoDB clients and server\n "
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/rw_data_guide/",
	"title": "Reading and writing",
	"tags": [],
	"description": "",
	"content": " Chapter 3 Reading and writing MongoDB Data  \u0026#39;MQL\u0026#39; vs. SQL  \u0026#39;MQL\u0026#39; vs. SQL\n The data: BSON documents  The JSON-like data format used in MongoDB\n "
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/credits/",
	"title": "Credits for the Learn Theme for Hugo",
	"tags": [],
	"description": "",
	"content": " Contributors Thanks to them  for making Open Source Software a better place !\n.ghContributors{ display:flex; flex-flow: wrap; align-content: flex-start } .ghContributors  div{ width: 50% ; display: inline-flex; margin-bottom: 5px; } .ghContributors  div label{ padding-left: 4px ; } .ghContributors  div span{ font-size: x-small; padding-left: 4px ; }   @matcornic 154 commits   @matalo33 37 commits   @lierdakil 16 commits   @coliff 15 commits   @gwleclerc 13 commits   @mdavids 10 commits   @ozobi 5 commits   @Xipas 5 commits   @pdelaby 4 commits   @Chris-Greaves 3 commits   @mreithub 3 commits   @massimeddu 3 commits   @dptelecom 3 commits   @willwade 3 commits   @denisvm 2 commits   @gpospelov 2 commits   @tanzaho 2 commits   @wikijm 2 commits   @lfalin 2 commits   @alexvargasbenamburg 1 commits   @afs2015 1 commits   @arifpedia 1 commits   @berryp 1 commits   @MrMoio 1 commits   @ChrisLasar 1 commits   @giuliov 1 commits   @haitch 1 commits   @ImgBotApp 1 commits   @RealOrangeOne 1 commits   @JohnBlood 1 commits   @JohnAllen2tgt 1 commits   @kamilchm 1 commits   @lloydbenson 1 commits   @sykesm 1 commits   @nvasudevan 1 commits   @654wak654 1 commits   @PierreAdam 1 commits   @ripienaar 1 commits   @EnigmaCurry 1 commits   @taiidani 1 commits   @exKAZUu 1 commits   @Oddly 1 commits   @shelane 1 commits   @tedyoung 1 commits   @Thiht 1 commits   @editicalu 1 commits   @fossabot 1 commits   @kamar535 1 commits   @nonumeros 1 commits   @pgorod 1 commits   @proelbtn 1 commits   And a special thanks to @vjeantet for his work on docdock, a fork of hugo-theme-learn. v2.0.0 of this theme is inspired by his work.\nPackages and libraries  mermaid - generation of diagram and flowchart from text in a similar manner as markdown font awesome - the iconic font and CSS framework jQuery - The Write Less, Do More, JavaScript Library lunr - Lunr enables you to provide a great search experience without the need for external, server-side, search services... horsey - Progressive and customizable autocomplete component clipboard.js - copy text to clipboard highlight.js - Javascript syntax highlighter modernizr - A JavaScript toolkit that allows web developers to use new CSS3 and HTML5 features while maintaining a fine level of control over browsers that don't support  Tooling  Netlify - Continuous deployement and hosting of this documentation Hugo  "
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/",
	"title": "Top page",
	"tags": [],
	"description": "",
	"content": " Akira's guide to MongoDB A guide aimed towards:\n Server / database administrators who will be running MongoDB servers, or Application developers who want a deep understanding of the performance, communication and high-availability behaviour of MongoDB clusters.\n  What's special about this guide compared to others?\n The chapter structure and topic emphasis is shaped by my experience working for MongoDB support. That is it is weighted according to what sort of questions and conceptual gaps I know are more common amongst professional MongoDB users, including those with many years experience of relational databases. The more verbatim details that we all forget (and have to look up later anyway) are not included here. Instead there will only be links to the right page for it in the official https://docs.mongodb.com/manual/ documentation site. Viva la internet! Driver API usage will be explained in tandem with Mongo Wire Protocol as it is the common reality underlying all of the drivers. It is also the key piece of the picture when understanding how and when client requests and the server responses enter and leave the mongod and mongos server processes.  "
}]