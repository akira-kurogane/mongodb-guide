[
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/stability_guide/the_replicaset/config_and_status/",
	"title": "Replica set configuration and status",
	"tags": [],
	"description": "",
	"content": " Config The replica set config is the BSON document in local db's system.replset collection. You usually see this running rs.conf() (= replSetGetConfig).\nFrom a code perspective it is better to say that the replica set config is the in-memory replica set configuration that is always synchronized between the live nodes, and system.replset collection is just the on-disk serialization. But as you will see db.system.replset.findOne({}, {\u0026quot;_id\u0026quot;: false}) and rs.conf() look the same.\nThere are a few replication options in the mongod configuration file, but except for the replica set name these do not overlap with the properties in the replica set config. If the name in the configuration file and the system.replset collection disagree the mongod will abort on startup, so I tend to see the configuration file copy of the name as a safety check only.\nModifying To update the replica set config there is no command that allows you to update just one value at a time. You must take a copy of the existing configuration, modify parts of that object, then send the whole document back in a rs.reconfig() (= replSetReconfig) command.\nStatus Replica set status (seen with rs.status() = replSetGetStatus) includes a \u0026quot;members\u0026quot; array with the hosts identified by hostname (or IP address) and port (as you get from rs.conf()) but is otherwise the living state that would disappear instantly with a mongod process's shutdown. The most important information to you is probably which the replication state the members are in. Next is how close the replication in secondaries is following the primary.\nReplication states PRIMARY, SECONDARY and ARBITER are the only running-as-normal replication states.\nThe others (STARTUP2, RECOVERY, UNKNOWN, ROLLBACK, DOWN, REMOVED, and the more-or-less never seen STARTUP) are error or transitional phases.\nWhile a node is running normally as a primary:\n Clients will send all write commands to it Clients will also send read commands to it if they have (the default) \u0026quot;primary\u0026quot;. \u0026quot;primaryPreferred\u0026quot; and \u0026quot;nearest\u0026quot; read preference reads may also be coming. At least one of the secondaries will be tailing its oplog. (Not necessarily all of them \u0026ndash; they usually do, but they can also tail from another secondary if it allows the replication to go faster.) It monitors the highest optime the fastest secondary has proceeded to, the maximum optime half (or more) of the secondaries have proceeded to by the information they pass back in the repeating getmore commands from the secondaries and the heartbeats. When it finishes applying writes with w:2+ or w:majority writeConcern against its own data it parks those in a queue and, for the meantime, stops working on them. Only when an asynchronous process tracking the progress for the relevant number of secondaries reaches the same opTime (or later) does the command properly complete, including sending the net response back to the client.  While a node is running normally as a secondary:\n It will reject all writes made by any user except the internal '__system' user It will reject reads unless 'slaveOk' field in the request is true. The replication thread(s) begin a find on the local db's oplog.rs collection with the clause that the \u0026quot;ts\u0026quot; field \u0026gt; the latest \u0026quot;ts\u0026quot; value they already have in their own oplog collection. This find will have the \u0026quot;tailable\u0026quot; and \u0026quot;awaitData\u0026quot; options set to true. Until there is an error this find will be continued with repeated getmore commands. The replication thread(s) apply the ops from the source using the (wait for it) applyOps command.  "
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/perf_guide/plan_optimization/",
	"title": "Improving your queries, updates and aggregations",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/monitoring_guide/serverstatus/",
	"title": "serverStatus command",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/expansion_guide/active_dataset_concept/",
	"title": "Active dataset - Key concept",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/stability_guide/the_replicaset/",
	"title": "Replica sets",
	"tags": [],
	"description": "",
	"content": " Downtime-proofing Replica sets have been around for a while before MongoDB. The older name for those was master-slave sets. Clients write to the one master node (or a node that is acting as master for those particular records) and the slaves copy the same updates.\nRelational databases that existed before MongoDB had to 'bolt it on' after their original design was already set. MongoDB and other distributed databases or key-value stores of the same generation, such as Cassandra and Redis, had the mechanics for replication built in from the start. (To be pedantic: they were in mind from the start and were programmed in during the beta-phase development.)\nThe original purpose of the older master-slave replication added onto relational databases is being a guarantee against data being lost if the master node crashes and can't be recovered.\nTo that other features were added:\n The ability to manually switch the nodes, making a slave take on the primary role and directing the client traffic to the new master. Even better- Automatic switching if there is sudden death of a master. Optionally directing reads to secondaries so the read load is spread around.  Replication in MongoDB had all of those things built in from the start. This better start has allowed things to evolve further too.\nReplication is also part of the assumed maintenance procedure. No downtime to do upgrades or config changes \u0026ndash; do rolling restarts where the new binary version or per-node config settings are changed one by one.\n"
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/rw_data_guide/mql_vs_sql/",
	"title": "&#39;MQL&#39; vs. SQL",
	"tags": [],
	"description": "&#39;MQL&#39; vs. SQL",
	"content": " Call a function; don't write a statement 1. There is no MongoDB query language The \u0026quot;L\u0026quot; in SQL is \u0026quot;Language\u0026quot;. A syntax that defines how you can write a sentence such as \u0026quot;SELECT usr_id, COUNT(*) AS count FROM TableA GROUP BY usr_id\u0026quot; that can be parsed to become query or update plans.\nMongoDB driver API's don't require that. They don't require you to 'program' a piece of text for the remote database server to parse (and potentially throw a syntax error on).\n2. Call a function and give it arguments. Depending on which language you are programming your client application in the API will be different, but the common thing is that you make all database requests by calling separate functions for the matching database commands.\nHere are some examples of doing the insert command, covering one OOP and one imperative style language:\n//Python my_collection.insert_one(doc) //C mongoc_collection_insert_one(my_collection_pointer, doc_pointer, NULL, NULL, \u0026amp;err);  The above use the API provided by PyMongo and mongoc MongoDB drivers. If you use Python or C you can see the syntax is very normal for them. Likewise the Java driver is idiomatic for Java, the NodeJS driver is idiomatic for NodeJS, the C# driver is idiomatic for C#, etc.\nA database-side command of xXxx will be called by executing the API function probably named xXxx(..) or Xxxx(..) or mongo_xxxx(..), etc. It might be a synonym too, e.g. \u0026quot;find\u0026quot; \u0026lt;--\u0026gt; \u0026quot;query\u0026quot; or \u0026quot;delete\u0026quot; \u0026lt;--\u0026gt; \u0026quot;remove\u0026quot;.\nSome of the functions in the driver API are extra wrappers that provide some kind of syntactic convenience. The insert_one(..) function and its partner insert_many([..]) are an example. Underneath both is just one command, insert, which requires that the document(s) being inserted be passed in an array. insert_one saves you the tiny bit of boilerplate work of instantiating an array to put your one document inside.\nIn one sense db and collection names are arguments too In the example above there are two arguments, although the OOP example especially makes it easy to miss. There is the collection that will be inserted to, plus the doc that will be inserted.\nA database namespace will always be in scope too. Database name is typically set when you created the db connection so it wont be in the function-calling code line, but the driver will be packing it in every request for you behind the scenes.\nAs a matter of semantics you might call both the database and collection scope or you might call them arguments.\nI would say: In the wire protocol packed format they are just string names (argument), but in the code they are scope.\nOn the driver side the scope is kept mostly for tracking which db name and collection to pack in requests. On the server-side the db or collection name strings received from a wire protocol request are immediately used to reopen or instantiate memory structures with pointers to existing collection names for a given db, pointers to existing indexes for a collection, etc.\n3. Receive the result Nothing new here for any who has programmed a database-using application before. When you call the function there will be a call over the network to the MongoDB server and for any given function in the API there will be one type of result.\nTo choose the simplest example this is the response from some deletes executed using PyMongo. The response confirms they ran OK; in the case of the second it shows that 5 documents matched the filter clause and were deleted.\n... \u0026gt;\u0026gt;\u0026gt; del_result = posts.delete_one({}) \u0026gt;\u0026gt;\u0026gt; del_result \u0026lt;pymongo.results.DeleteResult object at 0x7f7580fa9ec8\u0026gt; \u0026gt;\u0026gt;\u0026gt; del_result.raw_result {'n': 1, 'ok': 1.0} \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; del_result = posts.delete_many({\u0026quot;z\u0026quot;: \u0026quot;test string\u0026quot;}) \u0026gt;\u0026gt;\u0026gt; del_result.raw_result {'n': 5, 'ok': 1.0}  Next example: The find (or however it is spelled) function returns a cursor object which can be used to iterate the 0 or more BSON documents the server matched in the query. A find_one (a.k.a. findOne) doesn't oblige to you to first take the cursor reference, then enter a loop to get the BSON results. Instead the result is a single BSON document (or null), once again saving you a little boilerplate code for the common case when you know you should only have one result (or don't care about supplementaries).\nThe various result types that exist are an interesting topic ... no, they're not. I lied. Whichever driver API you're using it is intuitive, but unchallenging and uninteresting as a result. Cursors iterate documents, writes return write results, the 'list database / collection / index names' functions return a cursor of those names, count returns a integer, etc., etc.\nRPC-ish More like an RPC-using lib than a language People who call it \u0026quot;MQL\u0026quot; reminds us of the 19th century people who used the term \u0026quot;horseless carriage\u0026quot; for the newly-invented automobile. Terms from incompatible old concepts were recycled to describe the new ones.\nThe automobile was truly something new. But a MongoDB driver is not - it packs requests as command objects (your client program language's data -\u0026gt; BSON command object) and an 'X' command is executed through the matching function just for 'X' in the mongod server code. It's not generic enough to be an RPC, but it's something like that.\nThe 'language' is knowing the function reference You can't just put anything in the X(...) function of course. It expects certain input, some required and some optional, and without the required arguments the driver will reject it even before sending it. At compile time, if it's a compiled language.\nFor example an update command in it's canonical, MongoDB Wire Protocol and server-side format has the following fields:\n db + collection namespace (set via the scope of a collection object; a collection object pointer; etc.) An array of one or more composite update objects with these fields:  A filter object to find the document(s) to update The update modifications Optional: upsert true/false Optional: \u0026quot;multi\u0026quot; true/false Optional: collation Optional: filters controlling which nested array items can be affected  Optional: ordered processing only true/false Optional: writeConcern Optional: bypassDocumentValidation true/false  Whichever programming language you are using the MongoDB driver API for it will, in it's update function (or update_one and update_many functions), be setting these same required fields and optionally setting the optional fields.\nYour job as the MongoDB-using programmer 1. Memorize the basic, required arguments. E.g. that an update needs at least two arguments on top of it's collection namespace scope - one \u0026quot;filter\u0026quot; argument that finds the doc(s) to update; another to set the new value(s) in it.\n2. Get familiar with the object-packing format of arguments When an argument is a scalar value, whether a universal programming datatype such as a string, number, boolean, or MongoDB extended type such as Datetime, ObjectId, etc., its obvious how to pass that.\nBut when an argument is an object (such as query filter, update modification rule, shard zone tag range) that's not obvious because those design choices are arbitrary. (This applies to any language/system; this is not a MongoDB-specific issue).\n//mongo shell javascript example db.my_collection.find({\u0026quot;x\u0026quot;: {\u0026quot;$lt\u0026quot;: 100}}, {\u0026quot;_id\u0026quot;: true, \u0026quot;dept\u0026quot;: true})  The line above is equivalent to \u0026quot;SELECT _id, dept FROM my_collection WHERE x \u0026lt; 100\u0026quot;.\nThe command-delimted field name list \u0026quot;_id, dept\u0026quot; directly after the keyword \u0026quot;SELECT\u0026quot; in the SQL query becomes {\u0026quot;_id\u0026quot;: true, \u0026quot;dept\u0026quot;: true}. What was \u0026quot;x \u0026lt; 100\u0026quot; put after the \u0026quot;WHERE\u0026quot; keyword in the SQL becomes {\u0026quot;x\u0026quot;: {\u0026quot;$lt\u0026quot;: 100}}.\nLearning the right order of the arguments is trivial, but the choices made on how to represent filter clauses, field lists, update operations are not necessarily the first one you might guess.\nSo learn the query filter, field projection, update specification and index specification documents so you have fluent recall of them. The aggregation pipeline stages and operators are also deserve studying until you can at least remember $match, $project and $group fluently too.\nThe query filter, update specification and aggregation pipeline stages have more-commonly used operators (e.g. $in, $sum, $set) and less-commonly used ones (too many to list). For the less common operatos and other object types (e.g. zoning shard tag ranges) just look them up on the rare occasions you use them.\n3. Keep the optional stuff in your fuzzy memory On your first read do make an point of skimming all the options to find things that are novel or otherwise surprising. Leave those neat tricks (or gotchas) in your subconcious so it can guide you later.\nAdvanced study Notice when client-side is presenting a pretty picture differing from the server command reality Most of the client-side functions match up to a single server command with arguments that are exactly the same. rs.status() -\u0026gt; replSetGetStatus, db.createUser(...) -\u0026gt; createUser, etc. Sometimes it a different command name and\nBut in other cases, particularly for the most-used commands, there are differences in what you see with your programming language's MongoDB driver API vs. the db server-side command spec.\nE.g. 1. db.setLogLevel(1, \u0026quot;network\u0026quot;) -\u0026gt; { setParameter: 1, logComponentVerbosity: {\u0026quot;network\u0026quot;: 1} }\nE.g. 2. You might seen in tutorials that the insert, update and delete methods in your API's examples were being fed single documents. In truth their matching server commands (insert, update and delete) take arrays of the new insert / update specification / delete filter docs.\nYour (recent version) MongoDB driver has insert-one or insert-many, update-one or update-many, and delete-one or delete-many functions though.\nIf you're aware that these are really calling the same server-side command it'll prevent you from various worries you might have. For example don't be concerned that calling insert_many([new_doc]) with a single item in the array is going to have worse performance. It's exactly the same thing as insert_one(new_doc) to the server side.\n"
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/connecting_guide/human_ix/conn_string_uri/",
	"title": "Connection string URI",
	"tags": [],
	"description": "MongoDB connection URI syntax",
	"content": " Ladies and gentlemen, I present to you (drum roll) the standard MongoDB connection string URI:\nmongodb://[user[:password]@]host[:port][,host[:port]]*[/[database_name][?[conn_option[=value]][,conn_option[=value]]*]]\nAt this point I expect all readers fall into one of following three groups:\n 'Yuck. Punctuation vomit or what?' 'Ah, a URI like those ones for ODBC / Postgresql / MS SQL server / MySQL etc. ...' 'You've neglected the DNS/SRV seedlist format, moron'  If you are in group #1 I'm sorry but I can't 'sexy it up' in any way, no matter how hard I try. At it's simplest the URI you type is short and easy, but as you add authentication credentials and options it can't help becoming more and more verbose.\nIf you are in group #2 one special difference to note is that multiple host+port tuples can be accepted, instead of just one. More on this in the Replicaset host-list syntax section below.\nIf you are in group #3, cool, you're done, you don't need this page and you can move onto the next. (And I'll say I'm envious that you've been given enough privileges to add and modify SRV records on your DNS servers.)\nNon-URI formats You may have used (or will see on other sites) ways to connect without using the URI format. E.g. with the shell mongo --host myhost.my.domain:27018, or a code sample something like var conn = new MongoClient(\u0026quot;myhost.my.domain\u0026quot;, 27018)'.\nThese are just for legacy compatibility and/or to give some abbreviation. In reality those arguments will be immediately reformatted into a new 'mongodb://...' string and that is what the driver code will use.\nAs well as being deprecated the non-URI formats differ in syntax from one language driver to another. Let's just stop thinking about them a.s.a.p.\nExamples Basic / minimal To make a new connection a MongoDB client needs at least the two things any TCP connection requires - a hostname (or it's IP address) and a port.\nmongodb://myhost.my.domain:27017/\nWhat if host or port are wrong or the MongoDB server can't be reached because of a network problem? The TCP connection will never be established and the error message will be something along those lines. E.g. 'socket exception', and not 'MongoDB server failure'.\nAdding access credentials If the DB requires users to authenticate with username and password then add those too, delimited by \u0026quot;:\u0026quot; and suffixed with \u0026quot;@\u0026quot;.\nmongodb://akira:secret@myhost.my.domain:27017/\nIf you have tricky punctuation characters in your password that would wreck the URI parsing (i.e. \"/\", \":\", \"@\", or \"%\") encode those with [percent encoding](https://tools.ietf.org/html/rfc3986#section-2.1). E.g. \"EatMyH@t\" - \"EatMyH%40t\".  By default/convention the user authentication credentials are saved in the \u0026quot;admin\u0026quot; db on the server. This is the assumed default for mongodb connection URIs too, so you can leave it absent (as above) most of the time.\nBut if the \u0026quot;akira\u0026quot; user authentication credentials had been created in in the \u0026quot;orderhist\u0026quot; user databases then that db name is needed as shown below. The first format sets the starting db namespace (that commands such as find, db stats, etc. will act in) as \u0026quot;orderhist\u0026quot; and the auth source db is assumed to be the same. The second format allows for the starting db namespace to be something else.\nmongodb://akira:secret@myhost.my.domain:27017/orderhist\nmongodb://akira:secret@myhost.my.domain:27017/[some_other_db]?authSource=orderhist\nI do not recommended created user auth outside the \"admin\" db \u0026ndash; the above is just for reference in case you are accessing a non-conventional MongoDB cluster or replica set.  What if the user credentials are rejected (e.g. unknown username or wrong password)? The TCP socket connection will be established for a moment. Over the TCP connection the username and it's hashed password will be sent. If they fail the server will send the failure reply ('user unauthorized', etc.) in a MongoDB Wire protocol OP_REPLY (or rereply OP_MSG?), then close the socket immediately.\nQ. \u0026quot;What if the MongoDB server requires user authentication but the client fails to give username and password?\u0026quot;\nUnintuitively the TCP connection will be established and stay open! It will remain open to allow the client to send db user credentials. Any command other than authenticate or the ones drivers need for basic state detection (isMaster, hostInfo, etc.) will be rejected with an authorization error.\nConnection options For convenience the full syntax again:\nmongodb://[user[:password]@]host[:port][,host[:port]]*[/[database_name][?[conn_option[=value]][,conn_option[=value]]*]]\nAfter the \u0026quot;/\u0026quot; that follows host[:port] all the values are optional. The first is the user_auth_db_name (see above), then whether that db name is present or not put \u0026quot;?\u0026quot; before any other parameters. Delimit with an \u0026quot;\u0026amp;\u0026quot;, like in a HTTP URI.\nExample If the DB was configured to accept connections that use SSL network encryption then from the client side we add the \u0026quot;ssl=true\u0026quot; to instruct the driver to do that. If we want the driver to make a pool of at least 50 database connections that different threads in the application can share, then we could add \u0026quot;minPoolSize=50\u0026quot;.\nmongodb://akira:secret@myhost.my.domain:27017/?ssl=true\u0026amp;minPoolSize=50\nStaring in v4.2 there is general renaming of \"SSL\" as \"TLS\" in mongod/mongos server node and mongo shell options, and the MongoDB documentation in general ([link](https://docs.mongodb.com/master/release-notes/4.2/#add-tls-options)). It seems the connection string URI options are going to remain _ssl\\*_ though.  Most common options ... as I recall seeing / expect should be used.\n replicaSet. Invalid if connecting to a sharded cluster. But otherwise use this ensure you establish a replicaset connection that will failover in the event of a primary switch, rather than just having a standalone connection. authSource readPreference N.b. I'd recommend not using this; i.e. always use the default, which is doing reads only from primaries. But I know many users do secondary reads (I hypothesize a habit learned from completely different, non-db systems that have load balancers) and this is the option for setting that. w (write concern level) connectTimeoutMS retryWrites If there is a primary switch this will automatically retry once (but only once) any writes to the old primary that errored because it crashed / was stepped down in the middle of the first attempt.  The (one) connection parameter that can't go in the URI Using the \u0026quot;ssl=true\u0026quot; option in my example above requires me to point out that, unfortunately, one of the SSL options (but so far only this one) need to be passed to the client connection function outside of URI. SSL connections usually require a CA cert file and/or a client certificate file to be used, and you can't put a local filepath into an URI. (Well maybe you can, but this isn't standardizedyet.)\nA C++ driver example of adding an SSL client PEM file is shown below. The point is simply that the minimal case of needing just one line with the URI connection in it is no longer possible; we have to prepare and add an extra connection option alongside it when the client connection object is constructed.\nmongocxx::options::ssl ssl_opts{}; ssl_opts.pem_file(\u0026quot;client.pem\u0026quot;); mongocxx::options::client client_opts{}; client_opts.ssl_opts(ssl_opts); auto client = mongocxx::client{uri{\u0026quot;mongodb://host1/?..\u0026lt;other_parameters\u0026gt;..\u0026amp;ssl=true\u0026quot;}, client_opts};  Replicaset host-list syntax The MongoDB connection URIs above could be syntactically valid as HTTP URLs if we simply replace \u0026quot;mongodb\u0026quot; with \u0026quot;http\u0026quot;, but it isn't always so. There is one key HTTP-like rule-breaker which is necessitated by the fact that MongoDB typically uses replica sets. If a replica set has hosts hostA, hostB and hostC, each using the same port 27017, then the URI can accept them in a comma-separated list:\nmongodb://akira:secret@hostA:27017,hostB:27017,hostC:27017/?ssl=true\u0026amp;minPoolSize=50\nIf you only specify one host (let's say hostA:27017) that the driver can connect to it will automatically query for the full replica set configuration/status. After that it will be aware of all the other hosts, so it might seem unnecessary to specify multiple hosts to connect to. But imagine a time when hostA was shut down for maintenance, or had crashed, and the client has to establish a new connection.\nDefaults Most of of the URI parameters have default values.\nMost useful to know is that the hostname default is localhost, and the port default is 27017. It's these that make it possible for clients to connect without specifying any connection options at all. All you need to do is to set up a mongod or mongos process running on port 27017 with no user authentication on the same server as the client.\n"
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/stability_guide/the_replicaset/elections/",
	"title": "Replica set elections",
	"tags": [],
	"description": "",
	"content": "By far the most common cause of an election is when you are restarting mongod nodes yourself for maintenance reasons. Restarting nodes whilst doing a version upgrade for example.\nThe end result of an election is that one node will receive the most votes, or equal votes but then go higher by a tie-breaker mechanism, and it will 'step up' to the PRIMARY role. The amount of time could be milliseconds, but can also be up to 10 seconds plus the extra milliseconds for network lag and replica set config updating. (It could even a few more seconds above 10 seconds if all nodes are overloaded by high write load.) The election \u0026quot;term\u0026quot; number in the distributed replica set config is incremented too.\nOnly the members of the replica set itself participate in the election. With the exception that a user can instruct a primary to step down (which is only a trigger) clients or mongos nodes do not participate in an election at all. They will only react to what the nodes do (see automatic switching).\nThe members continual monitoring of each other's status (or notice a lack of response) is primarily by two types of network command.\n \u0026quot;heartbeat\u0026quot; commands The repeated getmore commands fetching the oplog  There will be an election when:\n A majority of the members lose contact with the primary node because:  (QUICK:) It was shut down, or (SLOW:) It crashed abruptly, or (SLOW:) The network is blocked or broken  (SLOW election:) The primary is too slow to respond (because load on its server is beyond capacity) (QUICK election:) The primary 'steps down' from that role  Because it is instructed to (i.e. rs.stepDown() was executed), or It stepped down from primary role autonomously because its egress network traffic to a majority of the other members was blocked even though the ingress traffic was still arriving. E.g. there was some 'firewall fun' where it could send to (and get responses back from) the other members but they could not do vice versa.  (QUICK election:) Any node detects that the current primary has a lower priority value than itself. (This can only happen when there has been replica set configuration modification. By default all nodes have the same priority.) (QUICK election:) After one election if a 'losing' secondary realizes it has caught up on the oplog faster than the newly elected primary can during its catch-up phase then the more up-to-date secondary will trigger another election so it can take over instead.  A new mongod node being added to a replicaset does not cause an election. If an election did happen at that time it would be for an indirect reason.\nAny nodes that missed the election will detect they are out of date by seeing that the term other nodes have in their replica set config is higher, and will accept that new config. And relinquish primary role immediately if they still thought they had it. This would be unlikely as they usually would have stepped down when they lost contact to the majority of the other replica set members, but it might have happened if that old primary's egress network connections remained opened whilst the ingress ones were blocked. (Another 'firewall fun' situation.)\n"
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/stability_guide/oplog_replication/",
	"title": "MongoDB Oplog",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/rw_data_guide/bulk_writes/",
	"title": "Bulk writes API",
	"tags": [],
	"description": "",
	"content": "TODO\n"
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/perf_guide/hardware_upgrade/",
	"title": "Provisioning more and/or better hardware",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/monitoring_guide/cloud_metrics/",
	"title": "Cloud metrics",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/secure_guide/authentication/",
	"title": "Enabling authentication",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/expansion_guide/hardware_increase/",
	"title": "Hardware - the simple answer",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/rw_data_guide/bson_format/",
	"title": "The data: BSON documents",
	"tags": [],
	"description": "The JSON-like data format used in MongoDB",
	"content": " MongoDB stores both user collections' data and also its system collections' data in one and only one binary format - the BSON format.\n   JSON equivalent Serialized form in hexadecimal byte values     \u0026nbsp; 0x3e = 62 (byte size of this document) as int32\n3e 00 00 00   { \u0026nbsp;   \u0026nbsp; \u0026quot;_id\u0026quot; : 7, (datatype 0x01 = double), (cstring \u0026quot;_id\\0\u0026quot;), (7 as a double floating-point value)\n01 \u0026nbsp; 5f 69 64 00 \u0026nbsp; 00 00 00 00 00 00 1c 40   \u0026nbsp; \u0026quot;instr\u0026quot; : \u0026quot;XYZ 3m\u0026quot;, (datatype 0x07 = string), (cstring \u0026quot;instr\\0\u0026quot;), ((not-cstring!) string len 0x07 in int32), (byte array \u0026quot;XYZ 3m\u0026quot; (6 bytes) + an extra null byte)\n02 \u0026nbsp; 69 6e 73 74 72 00 \u0026nbsp; 07 00 00 00 \u0026nbsp; 58 59 5a 20 33 6d 00   \u0026nbsp; \u0026quot;hval\u0026quot; : 904.72, (datatype 0x01 = double), (cstring \u0026quot;hval\\0\u0026quot;), (904.72 in a double)\n01 \u0026nbsp; 68 76 61 6c 00 \u0026nbsp; f6 28 5c 8f c2 45 8c 40   \u0026nbsp; \u0026quot;ts\u0026quot; : ISODate(\u0026quot;2019-07-21T01:12:15.348Z\u0026quot;) (type 0x09 UTC datetime), (cstring \u0026quot;ts\\0\u0026quot;), (UTC milliseconds since the Unix epoch in an int64)\n09 \u0026nbsp; 74 73 00 \u0026nbsp; f4 1e 16 12 6c 01 00 00   } \u0026nbsp;   \u0026nbsp; 0x00 datatype indicator for a BSON document\nN.b. the document type is the only one that has it's type indicator at the end.      Notes on the above example   - The \"cstring\" type used for key names means UTF-8 character string with a tailing null byte on the end. This means all valid UTF-8 strings _except_ the case of having a string of one or more null bytes, which is pointless as a key name value anyhow. - Even though the \"\\_id\" value above looked like an integer to our naked eye, mongo javascript shell made it a 64 bit double float. This happened because I created the document in the (javascript) mongo shell. Depending on your client language you may or not have a floating-point or integer-style value by default; in all client languages you can specify the type if you are more explicity. E.g. in the mongo javascript shell I could have created the \\_id value as new NumberInt(7), or NumberLong. - You probably worked this out: Little-endian byte order is used for all the basic types. I.e. everything that isn't a byte string/array.     shell example with bsondump of a single-doc collection   ```bash ~$ mongodump  --eval 'db.foo.insert({\"_id\": 7, \"instr\": \"XYZ 3m\", \"hval\": 904.72, \"ts\": new ISODate()})' WriteResult({ \"nInserted\" : 1 }) testrs:PRIMARY bye ~$ mongodump  -d test -c foo --out /tmp/dump 2019-07-21T10:13:54.137+0900\twriting test.foo to 2019-07-21T10:13:54.149+0900\tdone dumping test.foo (1 document) ~$ ~$ bsondump /tmp/dump/test/foo.bson {\"_id\":7.0,\"instr\":\"XYZ 3m\",\"hval\":904.72,\"ts\":{\"$date\":\"2019-07-21T01:12:15.348Z\"}} ~$ #bsondump's job is to print a JSON string representation. To make strictly ~$ # valid JSON it must use only the base JSON scalar datatypes. To do this it ~$ # follows MongoDB's \"Extended JSON\" rules such as using {\"$date\":\"...\"} for ~$ # an ISODate. This makes it look like \"ts\" was a nested object, but it was ~$ # scalar value - the BSON spec's 64-bit integer of UTC milliseconds. ~$ ~$ od -An -t x1 /tmp/dump/test/foo.bson 3e 00 00 00 01 5f 69 64 00 00 00 00 00 00 00 1c 40 02 69 6e 73 74 72 00 07 00 00 00 58 59 5a 20 33 6d 00 01 68 76 61 6c 00 f6 28 5c 8f c2 45 8c 40 09 74 73 00 f4 1e 16 12 6c 01 00 00 00 ~$ ~$ #The 0x3e (= 62) little-endian int32 at the beginning is the byte length of ~$ # the first doc. There is no size field/value for the whole collection at ~$ # the beginning of the dump file. ```   Please see bsonspec.org if you want to know the full specification.\nThe main point for database users is:\n BSON is a variable-length format that packs the key-value pairs one after the other in tuples of type + key name [+ byte length when not a fixed-length type] + value data. Correct deserialization requires consuming the datatype value, the keyname, and a size field if the datatype uses one. Once you have that you can calculate the offset to the end of the value data. The start of BSON data is always assumed to be an object - never an array or scalar by itself. MongoDB won't store anything except a document in the collection structure.  mongodump's output *.bson files likewise just start with a document. The next document begins in the byte immediately after the end of the previous. E.g. Imagine there is a mongodump-created file xyz.bson with ten's of thousands of documents, of size 25kb +/- 1kb. The first four bytes might be, say, the int32 value 26031. The 26031st byte will be a null byte. The 26032 ~ 26035th bytes will be the int32 value of the next document's byte size. Let's say it is 25938. The (26031 + 25938 =) 51969th byte will be final null byte of the second document. The next four bytes will be be size of the next document etc. As long as the dump is valid you can skip through the head to tail of the documents without parsing their content, til eventually there will be four bytes holding int32 value n, where n is exactly the remaining length of the file starting from the beginning of this last object's int32 size field.  For brevity no nested objects or arrays were shown in the example above, but they are also another key-value tuple in this serialization/deserialization algorithm. Type value 0x03 or 0x04 will indicate an (embedded) object or array respectively, then next four bytes will be an int32 for the total size (this size field and final null byte included). Arrays are packed just like objects, including having keys, but the spec insists the keys must be \u0026quot;0\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;, ... etc. To be honest the keys seem totally superfluous to me. Key names consume space in every document, even when all the documents in a collection have exactly the same ones. Use short key names to save space. Except for the required [\u0026quot;0\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;, ...] in arrays the format places no expectations/assumptions about which fields are included, or in which order they are serialized.  In practice BSON is the encoding of MongoDB and isn't used in any other software as popular as MongoDB yet. Nonetheless the BSON specification is one thing and MongoDB is another. There are some extra requirements that MongoDB places on any BSON object it will store in a collection:\n 16MB maximum size. The BSON specification places no upper limit on the size of data it encodes but the MongoDB database server and the drivers do. \u0026quot;_id\u0026quot; field: Exluding the oplog every document saved to a collection will have an \u0026quot;_id\u0026quot; field value. It is the primary key value. An _id value of an ObjectId() type will be given automatically by the driver (not the db server) if none is specified at by the user code above the driver API beforean insert.  BSON is also used by MongoDB to package commands and results being sent to and from the server using the wire protocol. The BSON command documents being sent by clients will have the command name as the first key (this is a fixed expectation of the mongod/mongos server nodes) and don't need an _id key as they don't represent a collection document. They might have a collection document embedded, eg. { \u0026quot;insert\u0026quot;: \u0026quot;mycollection\u0026quot;, \u0026quot;documents\u0026quot;: [ { \u0026quot;_id\u0026quot;: 999, .... } ], ... }.\nDatatypes JSON only supports the same datatypes that a Javascript tokenizer will handle. (If you are Javascript programmer you are no doubt thinking 'But what about other types such as Date?' Surprise- these are not covered by the JSON specification.)\n String (Null-terminated UTF-8) Number (JSON does not specify the binary format) Boolean Null Object Array  BSON extends to have these necessary datatypes that mostly any database would need:\nNumber types:\n int32 int64 uint64 Double (8-byte IEEE 754-2008 format) Decimal (16-byte IEEE 754-2008 format) Datetime (without timezone, i.e. assumed to be UTC always) Timestamp Generic binary data ObjectID (MongoDB uses this type. It would be called a GUID in some other databases that exist.)  BSON also include these (in my opinion) exotic-for-a-database-system datatypes\n Min key Max key Javascript code (As a UTF8 string, i.e. not in a compiled, runtime-executable format.) A few special 'binary types' Function (as a compiled, runtime-executable format????) UUID MD5  "
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/rw_data_guide/crud_gof/find_cmd/",
	"title": "The find command",
	"tags": [],
	"description": "",
	"content": " You know what find is about - it's how you read collection data. You send it with a query filter (like the WHERE clause of SQL) and it returns a cursor with documents matching that filter. If you omit a query filter it will return all documents.\nUnlike SQL SELECT query a find command can only operate on a single collection or view.\nExcluding the scope of the collection name the find command has no required arguments, only optional ones.\nIn practice the query filter argument is usually set - it's not often you want to read the entire collection. The field projection argument or the sort order argument are the next most commonly-used ones. After that are ones like limit, skip, collation and index hint that also in SQL. There are also ones that more specialist and/or don't have a SQL equivalent. E.g. setting batchSize, or using tailable and awaitData to make a tailable cursor.\nThe alternative to find The find command is not the only way to read documents from collection. The aggregation pipeline also returns documents, with the same cursor concept that a find provides.\nThe result from the classic find command:\ndb.collection.find(filter_arg, projection_arg, sort_order)  will be indentical to that of the following aggregation pipeline.\ndb.collection.aggregate([ {$match: filter_arg}, {$sort: sort_order}, {$proj: projection_arg} ])  The query engine of MongoDB will create an internal plan that is either identical, or logically identically at any rate. Exactly the same documents will be returned. It will also follow the same sort order.\nThe order of some the pipeline stages in the aggregation above don't matter, by the way. It could have been the $sort first, $match second. If the $proj doesn't remove any of the fields that the $match or $sort stages reference then any of the other four permutations would also be OK. Aggregation pipeline optimization will collapse logically equivalent pipeline links internally to the same plan the query engine.\nNo collection joins. Oh wait, yes, collection joins. It's not too hard to argue that NoSQL databases' biggest paradigm change was that they didn't support table ( / collection) joins.\nThis is still true of the find command. The aggregation pipeline on the other hand does support joins through the $lookup operator.\n"
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/perf_guide/shard_big_collections/",
	"title": "Shard your biggest datasets",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/monitoring_guide/cloud_alerts/",
	"title": "Cloud alerts",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/secure_guide/access_control/",
	"title": "Access control (a.k.a. Authorization)",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/expansion_guide/sharding/",
	"title": "Sharding for expansion",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/connecting_guide/cs_nw_details/drivers/",
	"title": "Drivers and &#39;the wire&#39;",
	"tags": [],
	"description": "Putting perspective on what a MongoDB driver is through a description of the shape and behaviour of your client requests (and server replies) as TCP traffic",
	"content": " From the perspective of the driver developer a MongoDB driver:\n Marshalls data from the language's native types into the format the MongoDB server requires (== a BSON payload proceeded by a few classically simple network fields in each packet's header area). Sends and receives that info, keeping track of which reply from a server matches which request.  When using a connection pool it also keeps a track of which thread the requests were sent from  Goes to error handling when there are network interruptions like abrupt socket closure and other TCP casuality situations Implements a lot of detail in the 'Meta' API driver specification for server discovery and monitoring (\u0026quot;SDAM\u0026quot;) and server selection.  From the application developer's perspective the MongoDB driver:\n Presents the database as an object you can push data in and pull data out of. The API provided is idiomatic for your language. E.g. where Java programmers run a find() method on a collection object, C driver users run a mongoc_collection_find() function that takes a mongoc_collection_t* pointer argument, etc.  To look at it from another side this is what the driver API doesn't do:\n Involve the application programmer in maintaining the TCP socket connections. Involve the application programmer in determining which remote servers are the current primaries (i.e. the one that the writes happen on first) Expose network packet data in the wire protocol format  Apart from the fact that you open a connection, and there can be exceptions thrown when a server crashes or the network is disconnected, there is limited expression in the API that the database is on a remote server. There are no network-conscious concepts the user must engage with such as 'queue this request', 'pop reply off incoming message stack', etc.\nMany drivers; one Wire Protocol Regardless of which driver you are using, at the Wire Protocol layer they are all the same fundamentally. If they are contemporary versions there's a good chance the BSON payload in each Wire protocol packet is identical excluding ephemeral fields like a timestamps.\nThe format of data in MongoDB Wire Protocol requests and responses is relatively simple, but it is a binary one and is far from being human-readable. The below comes from TCP payloads captured using tcpdump, manually unwrapped using command line tools od and bsondump according to the info in the MongoDB wire protocol documentation.\n   Example find in various APIs  MongoDB wire packet  mongod code     mongo shell db.foo.find({\u0026quot;x\u0026quot;: 99};\nPyMongo db.foo.find({\u0026quot;x\u0026quot;: 99})\nJava db.getCollection(\u0026quot;foo\u0026quot;).find(eq(\u0026quot;x\u0026quot;, 99))\nPHP $db-\u0026gt;foo-\u0026gt;find(['x' =\u0026gt; 99]);\nRuby client[:foo].find(x: 99) → OP_MSG\nlength=180;requestID=0x1b73a9;responseTo=0;opCode=2013(=OP_MSG type)\nflags=0x00.0x00\nsection 1/1 = {\n\u0026nbsp; \u0026quot;find\u0026quot;:\u0026quot;foo\u0026quot;,\n\u0026nbsp; \u0026quot;filter\u0026quot;:{\u0026quot;x\u0026quot;:99.0},\n\u0026nbsp; \u0026quot;$clusterTime\u0026quot;:{ ... }},\n\u0026nbsp; \u0026quot;signature\u0026quot;:{ ... },\n\u0026nbsp; \u0026quot;$db\u0026quot;:\u0026quot;test\u0026quot;\n} → mongo::FindCmd::run   (A cursor object with first batch results) ← OP_MSG (as a reply)\nlength=180;requestID=0xb5a;responseTo=0x1b73a9;opCode=2013(=OP_MSG type)\nflags=0x00.0x00\nsection 1/1 = {\n\u0026nbsp; \u0026quot;cursor\u0026quot;:{\n\u0026nbsp; \u0026nbsp; \u0026quot;id\u0026quot;:{\u0026quot;$numberLong\u0026quot;:\u0026quot;0\u0026quot;},\n\u0026nbsp; \u0026nbsp; \u0026quot;ns\u0026quot;:\u0026quot;test.foo\u0026quot;,\n\u0026nbsp; \u0026nbsp; \u0026quot;firstBatch\u0026quot;:[\n\u0026nbsp; \u0026nbsp; {\u0026quot;_id\u0026quot;:ObjectId(\u0026quot;5b3433ad88d64ee7afb5dc80\u0026quot;), \u0026quot;x\u0026quot;:99.0,\u0026quot;order_cust_id\u0026quot;:\u0026quot;AF4R2109\u0026quot;}\n\u0026nbsp; ]\n\u0026nbsp; },\n\u0026nbsp; \u0026quot;ok\u0026quot;:1.0,\n\u0026nbsp; \u0026quot;operationTime\u0026quot;:{ ... },\n\u0026nbsp; \u0026quot;$clusterTime\u0026quot;:{ ... },\n\u0026nbsp; \u0026quot;signature\u0026quot;:{ ... },\n\u0026nbsp; \u0026quot;keyId\u0026quot;:{\u0026quot;$numberLong\u0026quot;:\u0026quot;0\u0026quot;}}}\n} ← ↲    OP_QUERY and early generations An optional detour for those who knew the original Wire protocol messages (OP_QUERY, OP_INSERT, etc.) and are interested in what traffic looked like with these.\n  Expand me...   The above is latest-and-greatest OP_MSG format. At time of writing only the 3.6+ mongo shell and dev-branch drivers would be using it. In truth most driver versions are still being shoe-horned into the legacy **OP_QUERY** message type. Per its name OP_QUERY was meant to only be for queries, but was repurposed for mostly any type of request message. In its network packet fields it included a \"fullCollectionName\" field because queries always need a a db and collection name scope). But there are commands that don't have a collection scope (eg. replicaSetGetStatus, createUser) but don't have a dedicated wire protocol message type either. How to send them? The workaround for those cases was that \"$cmd\" was used as a _dummy collection name_ at the end of the \"fullCollectionName\" field. This workaround became so standard that it is even set this way for commands such as _find_ which do need a collection scope. You can see in the example below that the collection name \"foo\" has moved inside the BSON and is absent outside. | Legacy wire packet examples | | --------------------------- | | OP_QUERY\nlength=215;requestId=0x6633483;responseTo=0;opCode=2004(=OP_QUERY type)\nfullCollectionName=\"test.$cmd\" _//N.b. the dummy \"$cmd\" collection name_ numberToSkip=0;numberToReturn=0xffff\ndocument = \\{\n\u0026nbsp; \"find\":\"foo\",\n\u0026nbsp; \"filter\":\\{\"x\":99\\},\n\u0026nbsp; \"lsid\":\\{ ... \\},\n\u0026nbsp; \"$clusterTime\":\\{ ... \\},\n\u0026nbsp; \"signature\":\\{ ... \\},\n\u0026nbsp; \"keyId\":\\{\"$numberLong\":\"0\"\\}\\}\\}\n\\} | | OP_REPLY\nlength=301;requestId=0xbb8;responseTo=0x6633483;opCode=1(=OP_REPLY type)\nresponseFlags=0x08(=AwaitCapable)\ncursorID=0 _//important for getMore cmds that follow, if any_;\nstartingFrom=0;numberReturned=1\ndocument = \\{\n\u0026nbsp; \"cursor\"\n\u0026nbsp; \\{\n\u0026nbsp; \u0026nbsp; \"firstBatch\":[\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \\{\"_id\":ObjectId(\"5b3433ad88d64ee7afb5dc80\"), \"x\":99.0,\"order_cust_id\":\"AF4R2109\"\\}\n\u0026nbsp; \u0026nbsp; ],\n\u0026nbsp; \u0026nbsp; \"id\":\\{\"$numberLong\":\"0\"\\},\n\u0026nbsp; \u0026nbsp; \"ns\":\"test.foo\"\\},\n\u0026nbsp; \u0026nbsp; \"ok\":1.0,\n\u0026nbsp; \u0026nbsp; \"operationTime\":\\{ ... \\},\n\u0026nbsp; \u0026nbsp; \"signature\":\\{ .. \\}\n\u0026nbsp; \\}\n\\} | My way of looking at is: - In the beginning there were just collection editing or reading commands (query, insert, update, delete) and four wire packet types for those, plus a reply message type. The db+collection namespace was put in a network field, outside the BSON payload document. - Soon there many more command types that the database server accepted. A generic command wire packet format was needed. The existing drivers (that needed to be supported for some time) started using OP_QUERY overloaded for this purpose. - A generic command wire packet type OP_COMMAND was invented! And used by mongo shell v3.4(?) and between nodes in clusters and replica sets. But it didn't go mainstream. - Instead the OP_MSG type has become the new standard, to be used by 4.2? era drivers. Neither the collection name or database name is in the network header fields - they'll be in \"ns\" (namespace) inside the BSON payload instead.   Database command type You might have noticed that there's no primary / headlined / specially labeled value in the BSON command object that indicates what sort of command the client is sending.\nYou might be wondering 'Does the server run through a list of key-value pairs in fixed order until it gets a match?' (E.g. if (commandMessage.hasKey(\u0026quot;find\u0026quot;) then --\u0026gt; FindCmd:run(), else if commandMessage.hasKey(\u0026quot;update\u0026quot;) -\u0026gt; UpdateCmd::run(), etc. ....?).\nNope, a simpler mechanism is used. From util/net/op_msg.h:\nStringData getCommandName() const { return body.firstElementFieldName(); }  Take the key name from the first key-value pair. End of function.\nA lesson from this is that order in BSON can matter (at least to MongoDB). Important for driver developers, but not application programmers as the driver API will take care of this point for you.\nWhat it looks like to the programmer I don't want to re-invent the documentation wheel for this part. MongoDB's official documentation tutorials are good and cover many language samples in one page. Some links for a couple of types of operations:\n Document insert example (Python, Java, Node.js, PHP, C#, Perl, Ruby, Scala) Query example (Python, Java, Node.js, PHP, C#, Perl, Ruby, Scala)  "
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/monitoring_guide/third_party_systems/",
	"title": "Third party systems",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/secure_guide/network_encryption/",
	"title": "Network encryption",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/connecting_guide/human_ix/mongo_shell/",
	"title": "The mongo shell",
	"tags": [],
	"description": "The simple truth about the mongo shell",
	"content": " What it is The mongo shell is just another client application. There, I said it.\nIt is not a special node within a MongoDB replica set or cluster. It is an application that connects and communicates with the mongod (or mongos) nodes with the same MongoDB Wire protocol TCP traffic that any other application could. If it was a black box rather than being open source, you could reverse-engineer it even without super-dooper elite hacker skills. It has no special sauce that gives it elevated privilege or better performance compared to what any MongoDB driver-using application can have.\nWhat is unique about the mongo shell compared to the thousands of other MongoDB-connected applications you might install on your computer is that is an interactive CLI (command line interpreter) a.k.a. REPL (read-evaluate-print loop). It's not the only MongoDB CLI that has ever existed, but it is the only popular one to date.\nWhy use it Having an interactive shell is a practical requirement for doing administration, so basically everyone will use it for that reason at least. Most people will also use it for learning. The MongoDB documentation uses mongo shell syntax all over too.\nConnection examples On the unix (or windows) shell you can specify connection options, and optionally input (a script file to run or a single string to run).\nIf you are not already familiar with the command-line arguments the mongo shell accepts please expand the following section.\n  Expand me to see mongo shell connection examples   The examples beneath show how to connect to: - A replicaset named \"**merch_backend_rs**\" - It has two normal, data-bearing nodes running at - **dbsvrhost1:27017** (the current primary), - **dbsvrhost2:27017** (currently a secondary), - And an arbiter on a third host somewhere. - The main user database is \"**orderhist**\". - There is a user \"**akira**\" with password \"**secret**\", and the usual \"**admin**\" db is the authentication database (i.e. where the _system.users_ and related system collections are). Common usage forms shown below. See here for the all the options. ```sh # Most typical mongo --host dbsvrhost1:27017/orderhist -u akira -p secret --authenticationDatabase admin # Specify the replicaset name to guarantee a proper replset connection mongo --host merch_backend_rs/dbsvrhost1:27017,dbsvrhost2:27017/orderhist -u akira -p secret --authenticationDatabase admin # Using a mongodb URI connection string, the same as in your application code. mongo --host 'mongodb://akira:secret@dbsvrhost1:27017,dbsvrhost2:27017/orderhist?authSource=admin\u0026replicaSet=merch_backend_rs' # If you have disabled authentication in the mongod configuration, and it is # running on port 27017 on localhost, and you want to use the \"test\" db ... # Bingo!, the naked command will work. mongo # Execute a javascript script file mongo --host dbsvrhost1:27017/orderhist -u akira -p secret --authenticationDatabase admin daily_report.js # Execute a javascript statement as a command-line argument. mongo --host dbsvrhost1:27017/orderhist_db -u akira -p secret --authenticationDatabase admin --eval 'var acnt = db.collection_a.count(); var bcnt = db.collection_b.count(); if (acnt != bcnt) print(\"Reconcilliation error: Collection a and b counts differ by \" + Math.abs(acnt - bcnt));' ``` In the case of sharded cluster do _not_ add a replicaset parameter in the connection arguments. Just provide the hostname and port of the mongos node you are connecting to (or a comma-delimited list of them).   Internals Although it is made with C++ the language that this CLI interprets is Javascript. Apart from a very small number of legacy, imperative-style command expressions such as \u0026quot;show databases\u0026quot;, \u0026quot;exit\u0026quot;, etc. everything is Javascript.\nShell parsing The syntax exception: Legacy MySQL-like commands use \u0026lt;database_name\u0026gt; show databases show collections  Apart from \u0026quot;use database_name\u0026quot;, which sets the database namespace the client sends in the Wire Protocol requests, these legacy command expressions are all translated internally to a Javascript function. For example \u0026quot;show collections\u0026quot; is really:\n//The real code behind \u0026quot;show collections\u0026quot;: if (what == \u0026quot;collections\u0026quot; || what == \u0026quot;tables\u0026quot;) { db.getCollectionNames().forEach(function(x) { print(x); }); return \u0026quot;\u0026quot;; }  To see how these parsing exceptions are achieved you can look at the shellHelper.show function in mongo/shell/utils.js.\nPlain Javascript The mongo shell will process javascript with referring to any database context, if you want to. Below are some client side-only expressions and functions, pretty much identical to those you can do in the native Javascript supported in web browsers etc.\nvar x = 1; for (i = 0; i \u0026lt; 100; i++) { print(i); } function max(a, b) { return a \u0026gt; b ? a : b; }  Javascript that acts with database connection objects Unless you use the --no-db argument there will be the \u0026quot;db\u0026quot; special global object which can be used to send db command messages over the connection to a MongoDB server.\nuse \u0026lt;database_name\u0026gt; //set current database namespace db.version() //database namespace doesn't affect this particular command //Because I did not capture the result into a variable (i.e. I didn't put \u0026quot;var version_result = …\u0026quot; at the front) // the shell will capture the return value from db.getVersion() and auto-print it here 3.4.4 db.serverStatus() ///database namespace doesn't affect this particular command //As before, the return value from the statement will be auto-printed. db.serverStatus() { \u0026quot;host\u0026quot; : \u0026quot;myhost.mydomain\u0026quot;, \u0026quot;version\u0026quot; : \u0026quot;3.4.4\u0026quot;, \u0026quot;process\u0026quot; : \u0026quot;mongod\u0026quot;, \u0026quot;pid\u0026quot; : NumberLong(2175), ... ... \u0026quot;ok\u0026quot; : 1 } var cursor = db.\u0026lt;collection_name\u0026gt;.find({\u0026quot;customer_id\u0026quot;: 10034}); //this command is affected by the database namespace while (cursor.hasNext()) { var doc = cursor.next(); printjson(doc); } ...  In the example above:\n \u0026lt;database_name\u0026gt; is set as the db scope. This will go in command objects put into MongoDB Wire protocol messages sent from here. It won't be changed unless there is another \u0026quot;use xxxxx\u0026quot; statement or something that implies it, like a db.getSiblingDB(...) function. db.getVersion() will create a buildinfo command as BSON object. Through javascript-interpreter-to-C++-code boundary and then the C++ driver library that is put that in wire protocol message message and send it the db server. The response travels those layers in reverse, finally ending with the buildinfo result in Javascript object, from which the version property is picked and printed. db.serverStatus() is a helper function that executes _db.adminCommand({serverStatus: 1})) instead. I.e. this time the BSON object being packed and set is {serverStatus: 1} compared to {hostinfo: 1}. At the return the whole object (rather than just one scalar value property) is pretty-printed onto the terminal output. A similar pattern at first to the last two commands, with a {find: \u0026quot;database_name.collection_name\u0026quot;} BSON object being sent first. The result will contain the found docs, at least if they number 100 or less and fit within the max wire protocol message size. In that simple case it is one request, one response, end. But in the case not all of the documents are delived in one go the result will also contain a (cursor) \u0026quot;exhaust\u0026quot;: false value and cursor id value. The driver automatically continues fetching more results from server-side (assuming you bother to iterate the fetched first batch to its end) with a different type of command \u0026ndash; the getMore command (or in the legacy way, an OP_GET_MORE wire protocol message).  Ever-present db namespace The sent commands always includes a database namespace. You can change it at will (\u0026quot;use another_db_name\u0026quot;) so it is variable, but it can't be empty/null. Default is \u0026quot;test\u0026quot;.\nSome commands don't logically require a db namespace \u0026ndash; eg. isMaster, addShard, replSetGetStatus \u0026ndash; but they won't work unless it is set to \u0026quot;admin\u0026quot;. Many a time I've had those fail until I typed \u0026quot;use admin\u0026quot; and tried again. Some like isMaster you don't notice because you're probably never call it except by the a shell helper function (db.isMaster()) that sets it.\nCrystal ball gazing: Having said all this it isn't out the question that what is unnecessary will be removed in the future. The OP_MSG message format in particular doesn't require or even permit a db namespace in the network fields, so once older messages formats stop being supported some rationalization is possible. It will be interesting to see if the code base can handle having this trimmed out.\nExplicit db connection objects You don't have to use the \u0026quot;db\u0026quot; global var if you don't want to. You can manually create other live MongoDB connections objects with connect(\u0026lt;conn_uri\u0026gt;), or new Mongo(\u0026lt;conn_uri\u0026gt;) and give those whatever variable name you like. It would be an untypical way to use the mongo shell however.\nRecap To recap the mongo shell:\n Uses the MongoDB wire protocol to communicate with MongoDB servers the same as any application It is C++ internally Makes use of a javascript engine library and \u0026quot;readline\u0026quot;-style line editor library to provide a live Javascript command line interpreter / REPL. It doesn't handle the wire protocol 'raw' or control TCP primitives itself. It uses the standard C++ MongoDB client driver for that. Can be used to run Javascript code for the sake of Javascript alone, but the purpose is communicate with the database There is one \u0026quot;db\u0026quot; MongoDB connection object created which represents the connection to the standalone mongod or replicaset of mongod nodes or mongos host you specified with the --host argument when you began the shell. The behind-the-scenes flow every time you execute a db.XXX() command:  You create documents as Javascript objects, and execute Javascript functions in the interpreter. The mongo shell converts the Javascript objects to BSON, and the functions to known MongoDB server commands, which are also serialized in a BSON format. These include the argument values (if any), puts it into the OP_MSG request (or legacy OP_QUERY or the v3.2 experimental OP_COMMAND format requests) and sends it over the network The server responds with a reply in BSON The mongo shell shunts the BSON into the TCP payload to C++ object, then cast/marshalled to a javascript object through the Javscript engine. The converted-to-Javascript-binary-format result is assigned into a javascript variable if you set one, or auto-printed into the shell terminal if you did not.   Q. \u0026quot;But what about server-side Javascript? That's what MongoDB uses right?\u0026quot;\nNo, that's not what MongoDB uses. Well it can interpret and execute some javascript functions you send to it, but they're only for running within:\n a MapReduce command, or (Superseded by $expr in v3.6; removed v4.2): if using a $where operator in a find command, or (Deprecated v3.4; removed v4.2): as the \u0026quot;reduce\u0026quot;, \u0026quot;keyf\u0026quot; or \u0026quot;finalize\u0026quot; arguments in a group command.  These functions are javascript, but they get packed inside a special BSON datatype (just a string with a different enum value for type) to be sent to the server and the mongod is the only program I know that has ever been programmed to unpack that format. Being javascript it is a lot slower than the native C++ processing in the mongod process.\n"
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/rw_data_guide/crud_gof/",
	"title": "Queries, Inserts, Updates and Deletes",
	"tags": [],
	"description": "The classic gang of four",
	"content": " Translation table The most fundamental commands for reading and writing unfortunately have a slightly different names in different contexts. It's the same gang of four all the time though:\n   Context Query Insert Update Delete     CRUD names Read Create Update Delete   SQL equivalent SELECT INSERT UPDATE DELETE   mongod server-side commands find insert update delete   mongo shell find()\nfindOne() insert()\ninsertOne()\ninsertMany() update()\nupdateOne()\nupdateMany() remove()\ndeleteOne()\ndeleteMany()   pymongo driver find()\nfind_one() insert_one\ninsert_many update_one()\nupdate_many() delete_one()\ndelete_many()   mongoc driver mongoc_collection_find ..._insert ..._update ..._delete   Java driver find() insertOne()\ninsertMany() updateOne()\nupdateMany() deleteOne()\ndeleteMany()   (Mostly) extinct aliases query()  upsert*\nsave    Legacy Wire\nProtocol msgs OP_QUERY OP_INSERT OP_UPDATE OP_DELETE    Variations such as insertOne(), updateMany() are just syntactic sugar.\nE.g. 1. There is no such thing as an 'insertOne' command etc. in a wire protocol message. The driver will construct a { insert: \u0026lt;collection_name\u0026gt;, documents: [ ... ] } BSON command object that has only one document in the \u0026quot;documents\u0026quot; array.\nE.g. 2. An updateMany(\u0026lt;query\u0026gt;, u: \u0026lt;update\u0026gt;) function call will become a { update: \u0026lt;collection_name\u0026gt;, updates: [ q: \u0026lt;query_spec\u0026gt;, u: \u0026lt;update_spec\u0026gt;, multi: true ] } command.\nE.g. 3. An upsert is also really an update command like just above, but with a upsert: true flag present and set to true.\n The find command   The insert command   The update command   The delete command   Historical detour The documentation for the delete command says \u0026quot;New in version 2.6\u0026quot;. 'What! Was MongoDB an append-only database before then?'\nActually the same version info is written for the other two write commands (insert, update). And the find command has \u0026quot;New in version 3.2\u0026quot;!\nWhat this points out is that before v2.6 did not accept BSON command objects that were formatted like {\u0026quot;insert\u0026quot;: \u0026lt;collection_name\u0026gt;, ...}, {\u0026quot;update\u0026quot;: \u0026lt;collection_name\u0026gt;, ...}, {\u0026quot;delete\u0026quot;: \u0026lt;collection_name\u0026gt;, ...}. Instead the clients sent an OP_INSERT, OP_UDPATE or OP_DELETE mongo wire protocol message. The target collection name was up a level in a field in the wire protocol message object, whilst every other field was still in the same format in the BSON object packed within.\nThe difference may seem merely lexical, but the introducing a BSON format spec to allow an array of writes in each wire request (rather than just one) improved the way that bulk writes could be done.\n{\u0026quot;find\u0026quot;: \u0026lt;collection_name\u0026gt;, ...} likewise wasn't recognized by the server until v3.2. Before then queries were always sent in an OP_QUERY wire protocol message. The OP_QUERY wire protocol is still used a lot as a legacy workaround that won't be resolved until OP_MSG becomes standard. Until then a query might be a classic OP_QUERY with the target collection in the wire protocol field, or it might be an OP_QUERY with the fake \u0026quot;$cmd\u0026quot; name string in the wire protocol field for collection, and the packed BSON object will have \u0026quot;find\u0026quot;: \u0026lt;collection_name\u0026gt; as it first key-value pair (indicating that is the command name and what its scope is).\nFYI in v3.2 an OP_COMMAND message type was introduced in tandem with the packing of queries in a more generic command BSON object, but it never became mainstream. It was considered deprecated by v3.6 and is removed completely by v4.2.\n"
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/rw_data_guide/crud_gof/insert_cmd/",
	"title": "The insert command",
	"tags": [],
	"description": "",
	"content": " The insert command requires the collection name it will be operating on, and then just the new document (or documents - technically it accepts an array of them) that should go into the collection.\nIf the documents can't be inserted an error will be returned. As the insert command can contain multiple documents to insert the error field is not one error code and/or message, but rather the writeErrors array.\nWhen it succeeds the server response does not return the _id value of the document. But the MongoDB driver usually has saved it before being sent (maybe making a new one itself if it was not specified). So usually the driver can give it to you as property. See the section below for more details.\nErrors that incur can be access control ones (no write privilege on that collection), document validation rejection (if you are one of the relative few using them), writeConcern errors (such as might happen the moment secondaries are lost to the point of not having a majority in a replica set), etc. By far the most common too see is E11000 duplicate key error. This will arise both for duplicate values in user-made unique indexes and the compulsory primary key index on \u0026quot;_id\u0026quot;.\nThe _id primary key field Documents that lack an \u0026quot;_id\u0026quot; field will be given a new ObjectId as the _id value.\n Typically the MongoDB driver will generate the new ObjectId and add it before sending. There is no need that it be done server side. That's the beauty of the GUUID-style ObjectId datatype \u0026ndash; it's so easy to generate unique values. Even if the driver didn't create and add a new ObjectId value for _id then as a final line of policy implementation the server will.\nI think in this case you will have no information client-side about what the new _id value is, unless the insert was (opaquely to you) changed to be an update command in upsert mode and then getLastError command was used correctly (according to the oldest protocol conventions) and gets it from the upserted value there.  The server will also force that _id be the first field by BSON ordering if it wasn't already. I.e. if you had sent {\u0026quot;a\u0026quot;: true, \u0026quot;_id\u0026quot;: 999, \u0026quot;b\u0026quot;: false} you will find it is {\u0026quot;_id\u0026quot;: 999, \u0026quot;a\u0026quot;: true, \u0026quot;b\u0026quot;: false} when you get it back in a query.\nNo auto_increment data type / function If you don't like the GUUID-like ObjectIds and insted would prefer an unbroken chain of auto-incrementing integers as the primary key, sorry, that can only be performant in unpartitioned database designs and MongoDB does not provide it.\nWell, a performant way could exist for distributed databases that are always partitioned on the primary key, but MongoDB supports data partitioning by any secondary index as well. So n'th- and n+1'th-created documents can be on different shards. As an unavoidable result of allowing for that an implementation of an auto-incrementing integer value would have to use a software lock that waits out the responses from all shard primaries to make sure that the next integer isn't being 'double-booked' in 2+ shards simultaneously. That doesn't scale when you have a collection spread across multiple servers.\n"
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/monitoring_guide/currentop/",
	"title": "db.currentOp()",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/secure_guide/storage_encryption/",
	"title": "Storage encryption",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/connecting_guide/cs_nw_details/ha_connections/",
	"title": "Automatic failover",
	"tags": [],
	"description": "A light introduction regarding how MongoDB drivers will automatically respond to server failures",
	"content": "Although it hasn't been discussed in the book in detail so far, in the typical situation your client will be connected to a replica set, that is it will be sending writes to one node (the Primary) whilst also being connected to other nodes that are Secondaries. As long as the Primary is alive, accepting commands, and the other nodes continue to agree that it is the primary, that will continue without change.\nBut if the original primary node dies, or get cuts off network-wise from the other nodes, they will initiate a replica set election and one of them will become the new primary.\nThere be may some writes to the original primary that will have to be rolled back, particularly in an unbalanced network partition situation. That is an issue that means the rolled back operations will have to be examined manually and re-applied manually by administrators after the incident.\nThe election would take a fraction of a second usually (2 seconds at most), faster than any human is going to be able to react, so you want the application to automatically switch to use the new primary.\nQ. What do you program to make sure the client application send requests to the new primary, and avoid having every single thread in every single process throw endless repeating 'socket exception' or 'write rejected not primary' errors?\nA. Nothing. Because all the drivers have replica set automatic failover logic programmed into them you can't program (and don't need to program) anything extra to achieve re-routing to the surviving nodes.\nFrom the point of view of the application code using a MongoDB connection, database, or collection object it can continue to use the same object. The driver will route the reads and writes to the new primary. If you have explicitly requested reads to come from a non-primary node (possible with a feature called readPreference, to be discussed later) the connection for those will be changed away from the new primary node too.\nSo the challenge of staying connected is solved for you out of the box whichever driver you use.\nWhat is not handled is what to do with rolled back data. Rollbacks can occur in the case that the original primary doesn't step down immediately, and accepts some writes in the brief time before it determines it has lost the status updates from the other nodes and steps itself down. This will be explained more in the \u0026quot;Making MongoDB stable\u0026quot; chapter.\n"
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/stability_guide/driver_failover/",
	"title": "Driver automatic failover",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/secure_guide/non_mongodb_hardening/",
	"title": "Other &#39;hardening&#39;",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/rw_data_guide/aggregation_pipeline/",
	"title": "Aggregation Pipeline",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/rw_data_guide/crud_gof/update_cmd/",
	"title": "The update command",
	"tags": [],
	"description": "",
	"content": " In any database I can think of an update has two fundamental pieces of work to do \u0026ndash; a query to find the matching documents' place in the underlying storage, and the write modification that is done upon them.\nAccordingly the query filter and the object modification spec object are the two compulsory but separate arguments of a MongoDB update command. An example as mongo shell function:\ndb.cc_status_collection.update( { /* query filter: */ \u0026quot;client_code\u0026quot;: \u0026quot;R8THH\u0026quot;, \u0026quot;expiry\u0026quot;: {\u0026quot;$exists\u0026quot;: false } }, { /* update spec: */ \u0026quot;$set\u0026quot;: { \u0026quot;expiry\u0026quot;: ISODate(\u0026quot;2019-08-31T00:00Z\u0026quot;), \u0026quot;plevel\u0026quot;: 0 }, \u0026quot;$unset\u0026quot;: {\u0026quot;active_contacts\u0026quot;: \u0026quot;\u0026quot; }, \u0026quot;$inc\u0026quot;: { \u0026quot;admin_op_transaction_count\u0026quot;: 1 } } /* (other option fields exist; not used in this example) */ )  Query filter The update command's query filter (a.k.a. predicate) is exactly the same as the find command's.\nJust as with find it can be an empty object \u0026ndash; this means match any/all documents, as with a find command. Whether it matches just one or all depends on the \u0026quot;multi\u0026quot; flag discussed in the other options subsection below. find's equivalent is the \u0026quot;limit\u0026quot; option which can be any number, but update's multi is a boolean - 1 or all, no inbetween. N.b. options are not part of query filter or the object modification spec; they are set to the side of those two.\nObject modification spec The object modification spec cannot simply be the new object, or just some key-value pairs within it, in verbatim key-value pairs like the document given to an insert command. It must be a set of update operators which will objects that specify the target fields and the new value or increment amount or array modifications etc. that will be performed on them.\nHence rather than supplying just: \u0026quot;expiry\u0026quot;: ISODate(\u0026quot;2019-08-31T00:00Z\u0026quot;)\nYou must have a $set operation: \u0026quot;$set\u0026quot;: { \u0026quot;expiry\u0026quot;: ISODate(\u0026quot;2019-08-31T00:00Z\u0026quot;), ... }\n$set is the most fundamental update operator but it is only one of many:\n Approximately a dozen operators for updating scalar value fields. Approximately half a dozen operators for modifying nested arrays. Three 'operators' that provide a syntax for access specific values in nested arrays ($, $[], $[\u0026lt;identifier\u0026gt;]).  And there are about half a dozen operator modifiers (eg. $each, $sort). Documentation note: the modifiers are confusingly shown in the MongoDB document navigation list as operators, at least as of Aug 2019.\nUpdate operators are statements - you are instructing what to do with a field (or list of fields) with a mini-command, you might say.\n\u0026quot;$set\u0026quot;: { \u0026quot;expiry\u0026quot;: ISODate(\u0026quot;2019-08-31T00:00Z\u0026quot;), \u0026quot;plevel\u0026quot;: 0 },  Nearly every operator can iterate through a list of fields, doing the same thing for each. The \u0026quot;$set\u0026quot; operation above shows how that syntax works with two fields but it could be hundreds or thousands probably. Only $bit is an exception I believe \u0026ndash; if you needed to do it for multiple fields and a { \u0026quot;$bit\u0026quot;: ... } operation for each in the object modification spec.\nThere is no operator that will iterate all fields, or all fields matching a regex. The array reference operators provide some ways to iterate/find values regardless of which array index they are at, but as for key-value pairs you must supply the exact key names when you submit the update request.\nIn summary: the object modification spec is a sequentially-executed list of operations, each with a list of fields that will be modified, that will transform a copy of the old document to the new one.\nMulti, Upsert, Collation For every update there is a multi, upsert and collation option. This means that in a bulk updates you can mix and match these modes for each update separately.\nmulti\nIn short, if you were to use updateOne(...) in the mongo shell (or whatever driver API's equivalent) multi will be false. If you use updateMany(...) it will be true. If you use the legacy update(...) that existed before there were the new ...One() / ...Many() variants it will be false. I.e. if you leave the multi option unset the update commands only affects one (or zero) documents.\nupsert\nFrom the documentation: \u0026quot;Optional. If true, perform an insert if no documents match the query.\u0026quot;\nIf an upsert applies (i.e. no existing documents matched that query) the multi value has no relevance - one document will be inserted.\nThe way to tell if you command created a new document or not is to look in the \u0026quot;upserted\u0026quot; nested array that is returned. It will contain the _id value of the new document. If you have multiple updates in a single {\u0026quot;update\u0026quot;: ..., \u0026quot;updates\u0026quot;: [ ... ], ... } request you'll need to do matching to the original request to determine which ones exactly became upserts and which didn't. This is low-level (or at least semi-low) detail though. Possibly your driver does this automatically and you can access the original updates vs. new upsert _id idiomatically.\nOrdered batch writes, writeConcern, document validation The \u0026quot;ordered\u0026quot;, \u0026quot;writeConcern\u0026quot; and \u0026quot;bypassDocumentValidation\u0026quot; options are set at the same level as the array and hence apply to all the updates, if there are more than one. See batch writes for more information regarding \u0026quot;ordered\u0026quot;.\nPerformance As with a find command you should be sure there is an index that makes the query part efficient. There is no way (or need) to make the update part more efficient - once the real address to the document(s) is found the same amount of work is going to be performed.\nThe objection modification work might seem to be the bigger part of the work - it is usually the harder of the two main arguments to write, and it seems the software must take many more steps to execute than the query part.\nThe work to modify the BSON object is usually less time-consuming than the query part, however. Thousands of CPU steps can be performed faster in first-level cache than even one out-of-cache memory access to get a part of the storage engine's data structure (e.g. the next B-tree node) and that will probably be repeated several times before the existing document is fully accessed. And that's with a perfect index - think how high the cost will be if you have to document-scan N documents before finding the one to update.\n"
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/secure_guide/auditing/",
	"title": "Auditing",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/stability_guide/write_and_read_concern/",
	"title": "Read and Write concern",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/rw_data_guide/imp_exp_utilities/",
	"title": "Import and export utilities",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/rw_data_guide/crud_gof/delete_cmd/",
	"title": "The delete command",
	"tags": [],
	"description": "",
	"content": "delete is simple. The command requires the collection name it is operating on, and technically it can be multiple deletes in one command object (like insert and update), but after that the only argument you usually specify is the \u0026quot;q\u0026quot; (query) filter one. The only other options are limit (think of this as a boolean - 1 to limit to 1, 0 for limitation to be disabled) and the uncommonly-used collation option.\nLike insert and update one delete command accepts an array of delete statements,\nI really only want to point out two things:\n Set an empty filter argument and you will delete all documents. For some this is anti-intuitive: 'I specified nothing, nothing should be deleted' is the idea. Lose this idea! This is a filter - \u0026quot;filter nothing\u0026quot; == \u0026quot;do it to everything\u0026quot;. Deletes are not 'cheap'. They must do approximately the same I/O that an update command does. Removing the document from secondary indexes efficiently requires knowing the key values it was placed by in those indexes \u0026ndash; and the best way to get those is to read the entire document that you will soon 'destroy' and never read again.\nThe human concept for delete is 'forgetting about those documents'. In mental enery terms nothing could be easier. Instead of that, though, think of it as a clean-up that you must do right now, no excuses, before you do anything else.\nDropping a whole collection on the other hand is quick and 'cheap' for the size of data being released.  "
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/stability_guide/backup_choices/",
	"title": "Backup choices",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/stability_guide/restore/",
	"title": "Restoring backups",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/stability_guide/performance/",
	"title": "Performance stability",
	"tags": [],
	"description": "",
	"content": " This chapter's title implies that MongDB has a performance stability issue. It does not.\nMongoDB performance stability is primarily about application load stability. And this is true of all database's I've worked with.\nDBAs: you must not take responsibility EVER for application load instability. Unless it is your responsibility to be the development team's manager as well, in which case make sure you get that in writing and a pay rise to match.\nExceptions to the above Checkpoints When there are a lot of writes (think many GB/s per minute as a rough guide) there is variation in the MongoDB performance during checkpoints. Checkpoints are when the changes that have been made in memory are flushed to disk. If you get slow command latency appearing once per minute for a second or several, and you see disk utilization rising to 100% at the same time, this would be what it is.\nShard balancing One internal thing that can add real load to a given pair of shards is the movement of data (and deletion clean-up after) between shards to balance the number of chunks for sharded collections between them. (\u0026quot;chunks\u0026quot; are ranges of docs up to a certain size, default 64MB.)\nOne chunk being moved is barely noticeable. But when you shard a collection for the first time, or add a new shard, they will happen continuously for a long time. This means there will be batch inserts on the recipient shard(s) and deletes on the donating shard(s). A signficant impact if there is already continually heavy load from the users on the database.\nDon't overlook the cost of deletes, and be aware they get asynchronously scheduled to run after. ASAP if not too much load, but it could be delayed for a long time.\nTTL Indexes Two things to be aware of regarding time-to-live indexes\n Once per minute. The TTL index deletes are executed by a background thread that sleeps for 60 secs by default before iterating all indexes with the \u0026quot;expireAfterSeconds\u0026quot; field. If the deletes take, say, 0.3s in total each minute there will be 60.3s cycle of deletes appearing. This is a different cycle to checkpoints. Eventually they will overlap, then move apart, then move over again, etc. If a large number of docs match the TTL clause there will be no mercy - the mongod node will process the deletes for all them at once. And deletes are basically as costly as updates (they have to be - the documents have to be read to get the values that are the keys that should be removed from the secondary indexes). It is a common mistake to create a new TTL index to keep a collection smaller and not think that the first TTL 'pass' might be deleting a huge fraction of the collection in one go.  If you would like a much more constant cost and can accept a policy based on size rather than a timestamp-value limit, please use capped collections.\nThe OS performance is unstable This is not really a DB issue of course. But if your DB server is a shared physical server you may have other big applications competing for the same resources, affecting the mongod process's top processing rate.\nAnother form of OS instability is when the kernel or library performance is not even. This is rare given how well-written the Linux kernel is, but I can think of at least two known issues.\n Memory alloc libs manage large page lists - when there is large memory (say 256GB+) the number of pages can be large, and an occasional defrag of those pages can be heavily blocking to user processes until it completes. Filesystems are not all equal. Ext4 (or Ext3) are not as good as XFS, because they can also block when managing large number of file pages.  "
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/connecting_guide/human_ix/",
	"title": "How to connect from your app or a terminal",
	"tags": [],
	"description": "First things first: how you connect to the database service as a client app or administrator",
	"content": " Connection string URI  MongoDB connection URI syntax\n The mongo shell  The simple truth about the mongo shell\n "
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/guide_intro/",
	"title": "What you can do with MongoDB",
	"tags": [],
	"description": "A dash through the top reasons to use MongoDB",
	"content": " Develop faster Mostly everyone who has used one of the 'NoSQL' databases which came into popularity post-2010, of which MongoDB is the most popular, already appreciate this. But for those of you still using relational databases the key reasons for the higher development productivity are:\n Reduced lines of code You don't need to perform a full mental switch between the paradigm of your application's data structure v.s. the server-side data storage mechanism.  You are a glucose-powered computer and context change is expensive. Only so much glucose can be converted by your neurons into thought-sparks a day, so save your sparks for things that are more valuable to you.\nBroadest client language support Want to do things quicker by using more developers / tools? For example your Web front-end has been created in language X, but the data scientists that will mine gold from the big data need to use language Y? MongoDB has got you covered - C, C++, C#, Erlang, Go, Java, Node.js, Perl, PHP, Python, Ruby and Scala are supported.\nThe simplicity of the client API will also reduce the technical debt of your codebase significantly. To the original developer this may seem unimportant, but for the development department that owns the technology it is more important than the speed of new feature development.\nWrite and read data faster This has always been my favourite feature of MongoDB.\nLet's take a trip back in time, to 2010 or so. The first MongoDB version I was using was 1.6, and I had a task to load a dataset of some 10's of GB. I can't remember the client program, but it might have been mongoimport.\nAs the data load progressed:\n I was pleasantly surprised by the rate of document inserts. I was a little shocked by the rate of inserts. I was getting suspicious of the insertion rate. Was the counter just a client-side lie? (No, I confirmed server-side.) I started sanity-checking the numbers, by datasize too. I arrived at a figure ~30 MB/s. 30 MB/s reminded me of something. At the time commodity HDD specs advertised 30MB/s write speeds. I checked the server's disk specs. They were the same 30 MB/s rate.  The takeaway for me was the hardware specs were no longer some sort of fantasy numbers way above the user's reality, which was a given for RDBMS-using application developers until then. If the hardware manufacturers have engineered their equipment to flick x million or billion electrons per second from one silicon nano-suburb to another, I could now use all those electrons for my data purposes.\nWith the change in hardware-land to SSDs, and furthermore MongoDB's change to the WiredTiger storage engine, MongoDB users now enjoy throughput and latency somewhat higher than the puny 30MB/s figure above. But the key point is to expect MongoDB to redline your hardware's throughput and/or latency by the volume of your data being moved, without the database software eating a noticeable chunk of the server capacity for itself.\nGo big The growth of e-commerce and social networking in the noughties lead to many thousands of businesses having a problem that was limited to few before. This was dataset sizes that exceeded the capacity of the biggest server you could afford to buy. If your user base was large, well, you either became one of the few companies that engineered around this by getting good at distributed data in your server application, or you limited data detail and panicked about whether you'd last the next 2 months before the server RAM and disk upgrades were delivered.\nThe NoSQL databases for the most part came with a huge benefit for this businesses / websites - easy data partitioning. You don't program the distribution logic in your application, instead you leave it to the database driver or the db server node you connect to.\nThe NoSQL databases mostly all include replication, making automatic database failover another thing that happens on the other 'side' of the database driver.\nWhich field(s) should be chosen to partition data is still a very important decision that you need to make for yourself, but after that your MongoDB cluster will allow you to grow your data up to (Single server storage size) x (100's).\nGet bigger quickly Starting with a single, unsharded MongoDB replica set is not a problem if suddenly you find you data volume growing. A single replica set can be dynamically converted, in configuration, to being the first shard of a one-shard MongoDB cluster. With no downtime or any reinsertion of your user data you can gain the ability to add new shards. Add one, two, as many as are needed, and the first cluster's data will be redistributed automatically until the number of documents in collections is balanced between the shards.\nThis has no impact on the logical view of the data to the client. To the client it is as though there is one server with larger capacity. Even document data that might happen to be in the process of being moved from one shard to another as part of shard balancing will not experience an error or delay. (It will delay the background move instead, if the access is write).\nBe small You can also be small - MongoDB does not hang some performance-lowering burden of distributed data management on your database server, or place configuration burden on you as a database administrator, if you aren't using it.\nYou don't even have to have a replica set if, in a rare sort of use case, you can afford for your database to be down (say if a data center's power goes out) and furthermore don't care if the data is lost (say if the hard disk suffers irreversible corruption). MongoDB can run as a standalone mongod process and this lets you forgo the cost of having a second or third server. (Starting from v4.0 technically all nodes must be in a replica set by configuration, but that can be single-node replica set.)\nGet smaller quickly I'm just kidding. But if you weren't, yes, you can use remove (== document delete), drop (== whole collection drop), rs.remove(\u0026lt;replica member\u0026gt;) and removeShard.\nSurvive server, data center failures MongoDB replica set members contain copies of each other's data to within whatever limit of time it takes for an update on the primary to be replicated to the secondaries. This can be just a millisecond in the better cases.\nAn important corollary to this is: the drivers (i.e. the MongoDB API library you use in your code to connect to MongoDB) are all replicaset-aware, regardless of which language you are version.\nIf you are using sharding, you will connect to a mongos node. The mongos node does the failover handling in that case.\nSo if a server dies:\n The remaining replica set nodes will notice this (default is with \u0026lt;= 2 seconds, the 'heartbeat' cycle). If the server that died was the primary, they will hold a new election and one of the prior secondaries becomes the new primary. There is a dynamic replicaset state shared between them which is updated. When the former primary is restarted it will receive and act accordingly to that new state where it is a secondary, not try to continue as it was before it's halt. The clients that were connected, with the replica-set aware driver code, will detect the failure of the read/write they were performing at the time. Apart from the 'blip' of reads and writes that failed because they were en-route to the former primary just about the time it died, the application's database functionality remains up. E.g. if you were serving 10,000 web page request a minute some, say dozens, will have database errors. The web pages served before and after will be unaffected. You don't have to accept that the client fails to perform it's intended logic during the period of time between the first primary's crash and the new one stepping up. The type of error returned in the event of a lost primary will allow you to recognize that a new primary will shortly step up, and that you can try to do the same thing again (say with try-catch block). MongoDB drivers do not automatically retry for a good reason though - whether something should be retried or not depends on your application's requirements. So it is left to programmers to explicity choose what to do - retry, or not. Redo writes assuming the original context is still valid, or not. Just try the write again, and if there is a duplicate error you could assume the write originally succeeded (and was replicated to the secondary node that subsequently became the new primary)  Know the inside out Diagnostic information MongoDB includes diagnostic information.\n Log files db.serverStatus() Configuration:  db.cmdLineOptions() rs.status(), rs.conf() sh.status() The sharding config db.  currentOp and system.profile sampling documents  These are evidence you can examine to learn a lot about the state of your MongoDB instances.\nGraphical tools are not included in the normal server and client installation packages, but you can find them in MongoDB's cloud utlities, or third-party metric monitoring tools.\nSource code A nice feature, for the C++ programmers especially, is that the source code (excluding enterprise modules such as LDAP and Kerberos authentication, auditing, etc.) is publicly available.\n"
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/connecting_guide/cs_nw_details/",
	"title": "Client-Server network communication",
	"tags": [],
	"description": "The shape of network communication between MongoDB clients and server",
	"content": " Drivers and \u0026#39;the wire\u0026#39;  Putting perspective on what a MongoDB driver is through a description of the shape and behaviour of your client requests (and server replies) as TCP traffic\n Automatic failover  A light introduction regarding how MongoDB drivers will automatically respond to server failures\n "
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/connecting_guide/",
	"title": "Connecting to MongoDB",
	"tags": [],
	"description": " ",
	"content": " Chapter 2 Connecting to MongoDB  How to connect from your app or a terminal  First things first: how you connect to the database service as a client app or administrator\n Client-Server network communication  The shape of network communication between MongoDB clients and server\n "
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/rw_data_guide/",
	"title": "Reading and writing",
	"tags": [],
	"description": "",
	"content": " Chapter 3 Reading and writing MongoDB Data  \u0026#39;MQL\u0026#39; vs. SQL  \u0026#39;MQL\u0026#39; vs. SQL\n Bulk writes API  TODO\n The data: BSON documents  The JSON-like data format used in MongoDB\n Queries, Inserts, Updates and Deletes  The classic gang of four\n Aggregation Pipeline  \n Import and export utilities  \n "
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/stability_guide/",
	"title": "Making MongoDB stable",
	"tags": [],
	"description": "",
	"content": " Chapter 4 Making MongoDB Stable High availability\nMongoDB gives your application high availability through two features:\n Replication between replica set nodes, with automatic switching of the primary role when necessary. Automatic endpoint switching in the drivers  A related feature is:\n Tunable client-side preferences for write and read guarantees  "
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/expansion_guide/",
	"title": "Making MongoDB bigger",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/perf_guide/",
	"title": "Making MongoDB fast",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/secure_guide/",
	"title": "Making MongoDB secure",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/monitoring_guide/",
	"title": "MongoDB Diagnosis",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/docs_guide/",
	"title": "Read the docs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/credits/",
	"title": "Credits for the Learn Theme for Hugo",
	"tags": [],
	"description": "",
	"content": " Contributors Thanks to them  for making Open Source Software a better place !\n.ghContributors{ display:flex; flex-flow: wrap; align-content: flex-start }\n.ghContributors \u0026gt; div{ width: 50% ; display: inline-flex; margin-bottom: 5px; } .ghContributors \u0026gt; div label{ padding-left: 4px ; } .ghContributors \u0026gt; div span{ font-size: x-small; padding-left: 4px ; }\n  @matcornic 156 commits \n @matalo33 39 commits \n @coliff 17 commits \n @lierdakil 16 commits \n @gwleclerc 13 commits \n @mdavids 10 commits \n @ozobi 5 commits \n @Xipas 5 commits \n @pdelaby 4 commits \n @Chris-Greaves 3 commits \n @mreithub 3 commits \n @massimeddu 3 commits \n @dptelecom 3 commits \n @willwade 3 commits \n @diemol 2 commits \n @denisvm 2 commits \n @gpospelov 2 commits \n @jamesbooker 2 commits \n @tanzaho 2 commits \n @wikijm 2 commits \n @lfalin 2 commits \n @alexvargasbenamburg 1 commits \n @afs2015 1 commits \n @arifpedia 1 commits \n @berryp 1 commits \n @MrMoio 1 commits \n @ChrisLasar 1 commits \n @IEvangelist 1 commits \n @giuliov 1 commits \n @haitch 1 commits \n @ImgBotApp 1 commits \n @zeegin 1 commits \n @RealOrangeOne 1 commits \n @JohnBlood 1 commits \n @JohnAllen2tgt 1 commits \n @kamilchm 1 commits \n @lloydbenson 1 commits \n @massimocireddu 1 commits \n @sykesm 1 commits \n @nvasudevan 1 commits \n @654wak654 1 commits \n @PierreAdam 1 commits \n @ripienaar 1 commits \n @pocc 1 commits \n @EnigmaCurry 1 commits \n @taiidani 1 commits \n @exKAZUu 1 commits \n @Oddly 1 commits \n @shelane 1 commits \n @tedyoung 1 commits \n @Thiht 1 commits \n @editicalu 1 commits \n @fossabot 1 commits \n @kamar535 1 commits \n @nonumeros 1 commits \n @pgorod 1 commits \n @proelbtn 1 commits \n\nAnd a special thanks to @vjeantet for his work on docdock, a fork of hugo-theme-learn. v2.0.0 of this theme is inspired by his work.\nPackages and libraries  mermaid - generation of diagram and flowchart from text in a similar manner as markdown font awesome - the iconic font and CSS framework jQuery - The Write Less, Do More, JavaScript Library lunr - Lunr enables you to provide a great search experience without the need for external, server-side, search services... horsey - Progressive and customizable autocomplete component clipboard.js - copy text to clipboard highlight.js - Javascript syntax highlighter modernizr - A JavaScript toolkit that allows web developers to use new CSS3 and HTML5 features while maintaining a fine level of control over browsers that don't support  Tooling  Netlify - Continuous deployement and hosting of this documentation Hugo  "
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://akira-kurogane.github.io/mongodb-guide/en/",
	"title": "Top page",
	"tags": [],
	"description": "",
	"content": " Akira's guide to MongoDB A guide aimed towards:\n Server / database administrators who will be running MongoDB servers, or Application developers who want a deep understanding of the performance, communication and high-availability behaviour of MongoDB clusters.\n  The chapter structure and topic emphasis is shaped by my experience working for MongoDB support. That is it is weighted according to what sort of questions and conceptual gaps I know are more common amongst professional MongoDB users, including those with many years experience of relational databases.\nDriver API usage will be explained in tandem with Mongo Wire Protocol as it is the common reality underlying all of the drivers. It is also the key piece of the picture when understanding how and when client requests and the server responses enter and leave the mongod and mongos server processes.\n"
}]